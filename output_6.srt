1
00:00:00,570 --> 00:00:03,630
It. Let's use Nginx, maybe with a specific

2
00:00:03,700 --> 00:00:08,254
version. We can also specify a port here as

3
00:00:08,292 --> 00:00:11,726
you see in the usage. Again you can add ports and

4
00:00:11,748 --> 00:00:15,246
everything inside the configuration file, but you can

5
00:00:15,268 --> 00:00:18,794
also specify them right here so they end up in the configuration

6
00:00:18,842 --> 00:00:21,902
file already configured. So let's set a port,

7
00:00:22,036 --> 00:00:26,390
let's say it is 80 and replicas,

8
00:00:26,730 --> 00:00:30,486
which I'm going to copy from here. And this will be

9
00:00:30,588 --> 00:00:34,630
the main configuration of the deployment. And again

10
00:00:34,780 --> 00:00:38,334
we are telling Kubernetes or Kubectl

11
00:00:38,482 --> 00:00:42,474
do not actually execute this, we just want a preview in

12
00:00:42,512 --> 00:00:46,570
YAmL format and we want to save it as a deployment file.

13
00:00:48,430 --> 00:00:52,094
In my deployment yaml and with dry run

14
00:00:52,212 --> 00:00:55,280
without a value we get a warning here

15
00:00:55,890 --> 00:00:59,406
because I didn't specify the client value.

16
00:00:59,508 --> 00:01:03,182
That's just the warning. We should have our deployment file

17
00:01:03,326 --> 00:01:06,500
still created and let's actually check that.

18
00:01:06,870 --> 00:01:10,926
And there you go, we have the blueprint of deployment and pod

19
00:01:11,038 --> 00:01:15,006
configured with the image port, replica count

20
00:01:15,128 --> 00:01:18,566
and everything else. Basically you can set yourself and this

21
00:01:18,588 --> 00:01:22,294
could be super convenient if you don't want to go

22
00:01:22,332 --> 00:01:26,050
to the documentation and search documentation,

23
00:01:26,130 --> 00:01:29,862
but really quickly just want to generate a couple of configuration

24
00:01:29,926 --> 00:01:34,442
files and let's actually see final and third example for

25
00:01:34,496 --> 00:01:38,058
creating a configuration file for a pod. How do we

26
00:01:38,064 --> 00:01:41,386
do that? The way we create pods

27
00:01:41,418 --> 00:01:45,722
in the cluster, as you already saw, is not by Kubectl

28
00:01:45,786 --> 00:01:48,990
create, because if we check Kubectl create

29
00:01:49,060 --> 00:01:51,978
does not actually allow us to create pods,

30
00:01:52,074 --> 00:01:55,902
but it allows us to create all these components. So you can generate

31
00:01:56,046 --> 00:01:59,634
configuration files for all these resources here,

32
00:01:59,752 --> 00:02:02,850
but for pod we use Kubectl run

33
00:02:02,920 --> 00:02:06,562
command. So Kubectl run name of the pod,

34
00:02:06,626 --> 00:02:10,214
let's call this my pod and let's actually do help to see

35
00:02:10,332 --> 00:02:14,006
everything we can configure. So image which

36
00:02:14,028 --> 00:02:16,230
is the required attributes.

37
00:02:17,050 --> 00:02:20,970
Again let's use our well known Nginx

38
00:02:21,790 --> 00:02:25,658
and again here you have usage of different

39
00:02:25,744 --> 00:02:30,066
other options. So you can set ports here, you can add environment

40
00:02:30,118 --> 00:02:31,950
variables, you can add labels.

41
00:02:34,850 --> 00:02:38,558
So basically all this stuff that you see here

42
00:02:38,724 --> 00:02:42,786
and let's actually add some labels to

43
00:02:42,808 --> 00:02:49,634
the pod like

44
00:02:49,672 --> 00:02:53,700
this and dry run

45
00:02:54,470 --> 00:02:58,294
client Yaml outputs and let's save

46
00:02:58,332 --> 00:03:07,174
this in my pod yaml file and

47
00:03:07,212 --> 00:03:10,554
as you see we have a pod manifest with the

48
00:03:10,592 --> 00:03:14,198
image name of the container is same as the pod

49
00:03:14,294 --> 00:03:18,038
and we have the two labels that we specified on the command.

50
00:03:18,134 --> 00:03:22,138
So that's the second pro tip for working efficiently

51
00:03:22,234 --> 00:03:23,630
with kubernetes.

52
00:03:33,940 --> 00:03:37,424
Great. At this point we have a cluster where an

53
00:03:37,462 --> 00:03:41,396
application is running and accessible through its service.

54
00:03:41,578 --> 00:03:45,620
However, that Nginx application is only accessible within

55
00:03:45,690 --> 00:03:50,020
the cluster. However, if we deploy a web application, for example,

56
00:03:50,170 --> 00:03:54,440
we want end users outside the cluster to have access

57
00:03:54,510 --> 00:03:58,744
to it, right? It means we need to open our application to the

58
00:03:58,782 --> 00:04:02,184
outside world. And in this section we're going to see

59
00:04:02,302 --> 00:04:05,868
how to expose an application running inside a

60
00:04:05,874 --> 00:04:09,964
Kubernetes cluster to external users using

61
00:04:10,082 --> 00:04:13,116
three different ways. And the first one is

62
00:04:13,298 --> 00:04:16,604
using an external service called node

63
00:04:16,652 --> 00:04:17,250
port.

64
00:04:23,470 --> 00:04:27,370
We have an internal service, Nginx service

65
00:04:27,440 --> 00:04:31,082
with a cluster IP type. So cluster IP type

66
00:04:31,216 --> 00:04:34,830
means actually an internal service. So instead

67
00:04:34,900 --> 00:04:38,282
of an internal service, we're going to create an external

68
00:04:38,346 --> 00:04:43,758
service. So what I'm going to do is I'm going to actually delete service

69
00:04:43,844 --> 00:04:49,266
Nginx service from

70
00:04:49,288 --> 00:04:52,494
the cluster and it's

71
00:04:52,542 --> 00:04:56,802
gone. And I'm going to adjust the

72
00:04:56,856 --> 00:05:00,070
configuration of our service to turn it

73
00:05:00,140 --> 00:05:04,134
from an internal service to an external node port

74
00:05:04,252 --> 00:05:07,126
service, which is actually pretty simple.

75
00:05:07,308 --> 00:05:11,660
In the specification part of the service, we just

76
00:05:12,350 --> 00:05:16,122
provide a type. So by default the

77
00:05:16,176 --> 00:05:20,470
type attribute, if we do not specified in the configuration,

78
00:05:20,630 --> 00:05:24,606
it is going to be cluster IP, which is an internal service.

79
00:05:24,788 --> 00:05:28,174
However, we can override that default value and instead

80
00:05:28,212 --> 00:05:31,870
of cluster IP, we can make type node

81
00:05:32,930 --> 00:05:36,158
port. And there is one more thing we need to do here.

82
00:05:36,244 --> 00:05:39,938
Node port services actually need a third port

83
00:05:40,104 --> 00:05:43,506
in the port configuration of the service. So right here we

84
00:05:43,528 --> 00:05:47,602
have two ports. One of them is where the service is

85
00:05:47,656 --> 00:05:51,382
accessible at, and another one is where

86
00:05:51,436 --> 00:05:55,302
the pods or endpoint pods of the service are

87
00:05:55,356 --> 00:05:59,558
accessible. And Node port has a third port called

88
00:05:59,644 --> 00:06:00,760
a node port.

89
00:06:02,430 --> 00:06:05,834
So what is this node port and how does

90
00:06:05,952 --> 00:06:09,114
Nodeport service actually work?

91
00:06:09,232 --> 00:06:13,090
When we create an internal service for our pod,

92
00:06:13,270 --> 00:06:16,766
that service is accessible from other pods in

93
00:06:16,788 --> 00:06:20,590
the cluster at this specific IP address at the port

94
00:06:20,740 --> 00:06:23,838
of the service. When we create a node port,

95
00:06:24,004 --> 00:06:28,142
it actually creates an internal service, but in addition

96
00:06:28,206 --> 00:06:31,746
to that, it also opens ports on

97
00:06:31,768 --> 00:06:35,950
the nodes on each worker node in the cluster

98
00:06:36,110 --> 00:06:39,240
to make the service accessible from outside.

99
00:06:39,690 --> 00:06:43,046
And this port which gets opened on

100
00:06:43,148 --> 00:06:46,738
the servers, the Kubernetes worker nodes

101
00:06:46,834 --> 00:06:50,166
is called a node port. And there is actually a

102
00:06:50,188 --> 00:06:53,526
dedicated port range for these node

103
00:06:53,558 --> 00:06:57,222
ports. And you actually saw that range when we configured

104
00:06:57,366 --> 00:07:01,322
our AWS security groups. So going back to

105
00:07:01,376 --> 00:07:04,874
the security groups here in the worker nodes

106
00:07:04,922 --> 00:07:08,986
security group, we configured a firewall rule

107
00:07:09,098 --> 00:07:14,142
with the port range from 30,000 to 32,767.

108
00:07:14,196 --> 00:07:18,242
And we said that any IP address

109
00:07:18,376 --> 00:07:22,622
should be able to access this port on the worker nodes.

110
00:07:22,766 --> 00:07:26,498
And that's the port range that we use

111
00:07:26,664 --> 00:07:29,986
to open a node port on the worker nodes, when we

112
00:07:30,008 --> 00:07:33,734
create a node port service, and that means that the

113
00:07:33,772 --> 00:07:36,658
value of node port has to be within that range.

114
00:07:36,754 --> 00:07:40,486
So I'm actually going to take 30,000, which is the first in

115
00:07:40,508 --> 00:07:44,474
the range. You can also do 30,001, doesn't really matter,

116
00:07:44,592 --> 00:07:48,780
and let's save it and apply

117
00:07:53,410 --> 00:07:57,370
it. Nginx service created

118
00:07:57,450 --> 00:08:00,080
so now if I do Kubectl get service,

119
00:08:00,770 --> 00:08:04,350
you see that the type is not

120
00:08:04,500 --> 00:08:07,918
cluster IP anymore, but rather a node

121
00:08:08,014 --> 00:08:11,762
port. And you don't only have the service port

122
00:08:11,816 --> 00:08:15,134
here, but also 30,000, which is a node

123
00:08:15,182 --> 00:08:18,722
port. So the way it's going to work now is that the

124
00:08:18,776 --> 00:08:22,226
service, the node port service is actually now accessible

125
00:08:22,338 --> 00:08:25,526
from outside the cluster for end users on

126
00:08:25,548 --> 00:08:29,430
the IP address of the node, of the server itself and

127
00:08:29,500 --> 00:08:33,334
the node port. Right. So combination of these two is

128
00:08:33,372 --> 00:08:36,890
going to be the URL to access the service from outside

129
00:08:37,040 --> 00:08:40,314
and internally when the request comes in into one of

130
00:08:40,352 --> 00:08:43,354
those worker nodes. So this could be any one of them.

131
00:08:43,472 --> 00:08:46,638
The request will then be forwarded to the internal service

132
00:08:46,724 --> 00:08:50,334
IP address. So let's actually test that this works.

133
00:08:50,452 --> 00:08:54,414
I'm going to choose one of the worker nodes. It doesn't matter which one because

134
00:08:54,452 --> 00:08:57,906
the service is accessible on all the worker nodes. I'm going

135
00:08:57,928 --> 00:09:01,490
to copy the public IP address of the worker node

136
00:09:02,870 --> 00:09:06,306
and port 30,000. And there

137
00:09:06,328 --> 00:09:09,510
you go. We have a welcome to NgINx page,

138
00:09:09,660 --> 00:09:13,654
which means we were able to access the NgINx application

139
00:09:13,772 --> 00:09:17,798
or NgINx pods through the node port service

140
00:09:17,964 --> 00:09:21,954
externally from outside the cluster, again using

141
00:09:22,092 --> 00:09:25,962
the IP address of any of the worker nodes, it doesn't matter

142
00:09:26,016 --> 00:09:29,914
which one. And the node port or port that we

143
00:09:29,952 --> 00:09:34,106
opened on each one of the worker nodes by specifying

144
00:09:34,138 --> 00:09:36,480
that as a port in the service.

145
00:09:40,140 --> 00:09:44,004
Cool. So now our application is accessible externally,

146
00:09:44,132 --> 00:09:47,816
but we can't ask users to access the

147
00:09:47,838 --> 00:09:51,484
application on this kind of port and have

148
00:09:51,522 --> 00:09:54,428
to type in the IP address of the application,

149
00:09:54,594 --> 00:09:58,696
because this is not very user friendly. Also imagine if we had lots

150
00:09:58,728 --> 00:10:02,480
of applications that we exposed like this using

151
00:10:02,550 --> 00:10:05,916
a node port service. This would open up lots

152
00:10:05,948 --> 00:10:08,988
of ports on our worker nodes.

153
00:10:09,084 --> 00:10:12,192
And this is actually very insecure and also very

154
00:10:12,246 --> 00:10:15,684
messy because we will have to keep track of the node ports that we

155
00:10:15,722 --> 00:10:18,948
assign to the services. So there must be

156
00:10:19,034 --> 00:10:22,256
a better way of exposing our applications

157
00:10:22,368 --> 00:10:25,752
than using a node port service. So this is fine for

158
00:10:25,806 --> 00:10:29,796
testing, but there should be a better way of exposing

159
00:10:29,828 --> 00:10:33,192
our applications to the end users. Well,

160
00:10:33,246 --> 00:10:36,964
as a better alternative, we have another service type called

161
00:10:37,022 --> 00:10:41,020
a load balancer. Now let's see how load balancer service actually

162
00:10:41,090 --> 00:10:45,004
works. Going back to the cluster, I'm going to again

163
00:10:45,122 --> 00:10:49,070
delete the NgINx service

164
00:10:51,860 --> 00:10:55,820
in the cluster and go ahead and modify

165
00:10:55,980 --> 00:10:59,552
the Nginx service yaml file. And instead

166
00:10:59,606 --> 00:11:02,932
of type node port we're going to write

167
00:11:02,986 --> 00:11:06,564
here load balancer type.

168
00:11:06,682 --> 00:11:10,116
That's it. Everything else can actually stay the same.

169
00:11:10,218 --> 00:11:14,212
We have port of the service, target port and

170
00:11:14,266 --> 00:11:18,504
even the node port. So let's actually try to create this by

171
00:11:18,542 --> 00:11:25,464
applying and

172
00:11:25,582 --> 00:11:30,008
let's get the service. First of all, we have a type load balancer

173
00:11:30,104 --> 00:11:34,044
instead of node port here, and we also have an

174
00:11:34,082 --> 00:11:37,324
external ip which is pending and

175
00:11:37,362 --> 00:11:41,104
the same port mapping here as for the node port. So how

176
00:11:41,142 --> 00:11:44,304
does load balancer actually work? So the way

177
00:11:44,342 --> 00:11:48,064
the load balancer service actually works is that on top

178
00:11:48,102 --> 00:11:51,916
of the internal service that listens on the

179
00:11:51,958 --> 00:11:54,804
service port like 80, 80 here,

180
00:11:54,922 --> 00:11:58,964
and the node port service which opens the

181
00:11:59,002 --> 00:12:02,404
node port on the worker nodes, it creates a

182
00:12:02,442 --> 00:12:06,584
load balancer in front of the cluster nodes which

183
00:12:06,622 --> 00:12:10,436
will accept the request from outside and then load

184
00:12:10,468 --> 00:12:14,164
balance it to one of the worker nodes on the node

185
00:12:14,212 --> 00:12:17,476
port, which will then forward the request to the internal

186
00:12:17,508 --> 00:12:21,276
service IP address. So as you see, node port and load balancer do

187
00:12:21,298 --> 00:12:24,524
not replace the internal service type,

188
00:12:24,642 --> 00:12:28,732
but rather it builds on top of each other. So with load balancer we actually

189
00:12:28,786 --> 00:12:32,512
have all three types in one, which works

190
00:12:32,566 --> 00:12:36,044
in this hierarchical way. Now I said that load balancer

191
00:12:36,092 --> 00:12:39,692
gets created outside the cluster and is accessible

192
00:12:39,756 --> 00:12:42,964
at its own IP address and its own port.

193
00:12:43,082 --> 00:12:47,236
So the load balancer IP address or service

194
00:12:47,418 --> 00:12:51,008
itself is not created inside the cluster,

195
00:12:51,104 --> 00:12:55,028
it just gives you a service of that type. But the load balancer

196
00:12:55,044 --> 00:12:58,584
is actually a component outside the cluster, which means

197
00:12:58,702 --> 00:13:03,140
this external IP address does not come from the Kubernetes

198
00:13:03,220 --> 00:13:06,724
itself. This should actually be set by whoever

199
00:13:06,852 --> 00:13:10,476
creates a load balancer that sits in front of

200
00:13:10,498 --> 00:13:14,024
the cluster. So who is responsible for creating a load balancer

201
00:13:14,072 --> 00:13:17,336
and assigning it an IP address? Load balancer

202
00:13:17,368 --> 00:13:21,200
is actually created by cloud platforms that

203
00:13:21,270 --> 00:13:24,480
offer kubernetes as a managed service.

204
00:13:24,630 --> 00:13:28,384
And that is a limitation of a load balancer, because if we

205
00:13:28,422 --> 00:13:31,832
are creating a cluster on bare metal servers

206
00:13:31,996 --> 00:13:35,920
or just on empty self managed servers

207
00:13:36,000 --> 00:13:40,272
and basically install it from scratch instead of using a managed

208
00:13:40,416 --> 00:13:43,764
service of cloud platform, you don't

209
00:13:43,802 --> 00:13:47,128
have a load balancer or you don't get a load balancer out of the

210
00:13:47,134 --> 00:13:50,168
box. You would actually have to create your own.

211
00:13:50,334 --> 00:13:53,624
In this case because we installed the cluster from

212
00:13:53,662 --> 00:13:58,228
scratch on bare servers. And this will be the same for installation

213
00:13:58,324 --> 00:14:01,996
on on premise servers without any cloud platform,

214
00:14:02,178 --> 00:14:06,028
we will have to create a load balancer for each service

215
00:14:06,194 --> 00:14:09,824
that we want to expose. So that means if I go to my

216
00:14:09,862 --> 00:14:13,356
AWS account and search for a load

217
00:14:13,388 --> 00:14:16,770
balancer which is actually in the EC two service,

218
00:14:17,460 --> 00:14:21,484
if you scroll all the way down, you have load balancers section

219
00:14:21,532 --> 00:14:24,288
here which currently is empty.

220
00:14:24,384 --> 00:14:28,416
So when you're using a managed Kubernetes service like eks

221
00:14:28,608 --> 00:14:32,400
for example on AWS, the cloud native

222
00:14:32,480 --> 00:14:35,640
load balancer will be created for you

223
00:14:35,710 --> 00:14:39,528
automatically when you create a load balancer service.

224
00:14:39,694 --> 00:14:42,712
And you would see that right here. In our case,

225
00:14:42,766 --> 00:14:46,744
however, load balancer does not get created automatically, but we

226
00:14:46,782 --> 00:14:49,932
could create it manually and see how that works.

227
00:14:50,066 --> 00:14:54,248
Which means you would have to create your own load balancer, whether it's

228
00:14:54,344 --> 00:14:58,280
on premise solution or here on AWS

229
00:14:58,360 --> 00:15:01,328
itself. So just to demonstrate that,

230
00:15:01,414 --> 00:15:05,644
I'm actually going to create a load balancer for our load balancer

231
00:15:05,692 --> 00:15:06,290
service.

232
00:15:09,220 --> 00:15:13,652
So let's do create load balancer and this is going to be a simple application

233
00:15:13,786 --> 00:15:17,910
load balancer for HTP, which is our Nginx service.

234
00:15:18,520 --> 00:15:22,052
Let's do create, let's just give it a name.

235
00:15:22,186 --> 00:15:24,788
Test load balancer service.

236
00:15:24,954 --> 00:15:28,888
It has to be Internet facing because we are testing access

237
00:15:28,974 --> 00:15:32,584
from the browser and we

238
00:15:32,622 --> 00:15:35,944
have to select a zone and we

239
00:15:35,982 --> 00:15:39,808
actually have to select the zone where our nodes

240
00:15:39,844 --> 00:15:43,308
are. Again, remember, load balancer will sit outside

241
00:15:43,394 --> 00:15:47,240
the cluster in front of it and will get all the requests

242
00:15:47,320 --> 00:15:50,604
and then forward it, or load balance it to one

243
00:15:50,642 --> 00:15:54,384
of the worker nodes on the node port, which will

244
00:15:54,422 --> 00:15:58,368
then forward it to the internal service, which means we want

245
00:15:58,534 --> 00:16:02,108
our load balancer in the same zone as the worker nodes.

246
00:16:02,204 --> 00:16:05,936
So let's choose one b but because it's a load balancer

247
00:16:05,968 --> 00:16:09,568
we have to select at least two subnets.

248
00:16:09,664 --> 00:16:13,588
Now this is very specific to AWS, so you don't have to 100%

249
00:16:13,674 --> 00:16:17,492
understand how this works, especially if you're deploying Kubernetes

250
00:16:17,556 --> 00:16:21,224
cluster on a different platform. But just know

251
00:16:21,262 --> 00:16:25,060
that here we have to select at least two subnets

252
00:16:25,220 --> 00:16:28,936
of the VPC and one of them where the worker nodes

253
00:16:28,968 --> 00:16:32,172
are running and click next

254
00:16:32,306 --> 00:16:35,564
we have a warning about security settings. Then we

255
00:16:35,602 --> 00:16:39,464
just choose a default security group here and configure

256
00:16:39,512 --> 00:16:42,624
routing. So this is the part where we decide where

257
00:16:42,662 --> 00:16:45,904
does load balancer forward the request that

258
00:16:45,942 --> 00:16:49,632
it gets. Again one of the worker nodes on port

259
00:16:49,766 --> 00:16:53,152
30,000 which is the node port here,

260
00:16:53,206 --> 00:16:56,356
right? So this is the port which is already

261
00:16:56,458 --> 00:17:00,256
open on all the worker nodes because we deployed load

262
00:17:00,288 --> 00:17:03,604
balancer type of service. And we can check

263
00:17:03,642 --> 00:17:07,572
that again if I reload this. You see that NgInx

264
00:17:07,716 --> 00:17:11,208
is still accessible using the worker node ip address

265
00:17:11,294 --> 00:17:14,484
and 30,000. So on the EC two instances,

266
00:17:14,612 --> 00:17:18,100
port 30,000 and let's give this

267
00:17:18,190 --> 00:17:21,020
target group or routing group a name.

268
00:17:21,170 --> 00:17:24,312
Let's also call this test routing.

269
00:17:24,456 --> 00:17:29,000
And that's it. So this will basically say to instance

270
00:17:29,160 --> 00:17:32,176
types on port 30,000.

271
00:17:32,358 --> 00:17:35,664
And now we have to say which instances these

272
00:17:35,702 --> 00:17:39,616
are exactly. In our case, these are the worker nodes, only the

273
00:17:39,638 --> 00:17:43,684
worker nodes, not the master on port 30,000.

274
00:17:43,882 --> 00:17:46,576
And we do add to registered.

275
00:17:46,688 --> 00:17:49,956
So now we have a setup where we put

276
00:17:50,058 --> 00:17:53,764
a load balancer in front of the cluster as part

277
00:17:53,802 --> 00:17:57,304
of AWS. And this load balancer gets

278
00:17:57,342 --> 00:18:01,096
the request from the users and it will forward it to one of

279
00:18:01,118 --> 00:18:04,568
the two worker nodes on port 30,000.

280
00:18:04,734 --> 00:18:08,228
And with this configuration we do review

281
00:18:08,334 --> 00:18:11,896
and create load balancer.

282
00:18:11,928 --> 00:18:15,948
Provisioning will take some time because it needs to get an IP address,

283
00:18:16,034 --> 00:18:18,856
it needs to connect to those instances,

284
00:18:18,968 --> 00:18:22,304
need to basically configure everything in the background and

285
00:18:22,422 --> 00:18:25,840
after a couple of minutes it will go from provisioning state

286
00:18:25,910 --> 00:18:27,490
to active state.

287
00:18:30,420 --> 00:18:33,716
Once the load balancer is in an active state, it means that we

288
00:18:33,738 --> 00:18:37,492
can access the service in our cluster with

289
00:18:37,546 --> 00:18:41,124
this load balancer. Now how do we access the

290
00:18:41,162 --> 00:18:44,468
load balancer? Our load balancer has an IP address,

291
00:18:44,554 --> 00:18:47,816
but it also has a domain name, which is

292
00:18:47,998 --> 00:18:51,060
this one right here. So if we copy

293
00:18:51,140 --> 00:18:54,360
the name of the load balancer and

294
00:18:54,510 --> 00:18:57,920
paste it here on default port 80,

295
00:18:58,100 --> 00:19:01,692
that's where load balancer is accessible. And enter.

296
00:19:01,826 --> 00:19:05,292
There you go. We have welcome to NginX page

297
00:19:05,426 --> 00:19:09,048
because in the background the load balancer

298
00:19:09,144 --> 00:19:12,492
got the request and it then forwarded the request

299
00:19:12,636 --> 00:19:15,580
to one of its registered targets,

300
00:19:15,740 --> 00:19:18,816
worker node one on port 30,000

301
00:19:18,998 --> 00:19:22,528
or worker node two on port 30,000

302
00:19:22,694 --> 00:19:25,924
on their IP addresses. And you can actually ignore the health

303
00:19:25,962 --> 00:19:29,456
status here because we don't expose health information in the EC

304
00:19:29,488 --> 00:19:32,544
two instances and that's why it says unhealthy.

305
00:19:32,672 --> 00:19:36,168
So everything works and our load balancer is

306
00:19:36,254 --> 00:19:40,180
correctly load balancing the request

307
00:19:40,340 --> 00:19:44,244
to the node port all the way to internal

308
00:19:44,372 --> 00:19:45,050
service.

309
00:19:48,220 --> 00:19:51,532
Now as I said, when you're configuring a self

310
00:19:51,586 --> 00:19:55,224
managed kubernetes cluster, you would have to create the load balancer

311
00:19:55,272 --> 00:19:58,716
yourself. On a cloud platform like AWS you

312
00:19:58,738 --> 00:20:02,204
can provision a load balancer just like we did or

313
00:20:02,242 --> 00:20:05,868
on premise servers. You would have to implement or deploy

314
00:20:05,964 --> 00:20:09,424
some load balancer application. However, even if

315
00:20:09,462 --> 00:20:12,704
we were using a managed kubernetes cluster that

316
00:20:12,822 --> 00:20:16,084
automatically created load balancers for us,

317
00:20:16,202 --> 00:20:19,876
whenever we create a load balancer service, it would still have a

318
00:20:19,898 --> 00:20:23,220
disadvantage that we would end up with

319
00:20:23,370 --> 00:20:27,136
bunch of load balancers that each become an entry point

320
00:20:27,258 --> 00:20:30,772
to the cluster. So if we had hundreds of applications

321
00:20:30,836 --> 00:20:34,356
with hundreds of services, we would basically end up with bunch

322
00:20:34,388 --> 00:20:38,064
of load balancers that also need to be configured

323
00:20:38,132 --> 00:20:41,484
and mapped to our actual application's domain name,

324
00:20:41,522 --> 00:20:45,436
right? Because eventually people want to access our application

325
00:20:45,618 --> 00:20:49,708
on a proper domain name like myapp.com

326
00:20:49,874 --> 00:20:53,792
and not the domain name of the load balancer or IP address

327
00:20:53,846 --> 00:20:57,100
of the load balancer. So with hundreds of load balancers,

328
00:20:57,180 --> 00:21:00,880
how do we map each load balancer IP address to

329
00:21:01,030 --> 00:21:04,404
a proper domain name? And also each load balancer will

330
00:21:04,442 --> 00:21:08,436
expose a new node port on the server. So we

331
00:21:08,458 --> 00:21:11,824
have multiple issues here. With multiple

332
00:21:11,952 --> 00:21:15,044
node ports being exposed on all

333
00:21:15,082 --> 00:21:18,232
the worker nodes, it means we need to keep track of those

334
00:21:18,286 --> 00:21:22,216
ports. We also have multiple load balancers, which on a

335
00:21:22,238 --> 00:21:26,024
cloud platform would translate to a very high bill because each

336
00:21:26,062 --> 00:21:29,656
load balancer service will cost some money. And on top of

337
00:21:29,678 --> 00:21:33,676
that, we don't have one domain that will let us access all

338
00:21:33,698 --> 00:21:37,180
those load balancer services. So basically we would need

339
00:21:37,250 --> 00:21:42,336
a load balancer that will get all the traffic for myapp.com

340
00:21:42,438 --> 00:21:45,996
domain, and then based on a subdomain

341
00:21:46,108 --> 00:21:50,352
or a path in the URL, we'll redirect that

342
00:21:50,406 --> 00:21:54,304
request to one of the load balancers.

343
00:21:54,432 --> 00:21:58,272
And finally, you also need a way to secure

344
00:21:58,336 --> 00:22:02,384
access to your applications, right? So you need this htps

345
00:22:02,512 --> 00:22:06,268
configuration to access applications inside your cluster.

346
00:22:06,384 --> 00:22:10,788
So if you're installing and managing cluster on bare

347
00:22:10,804 --> 00:22:15,012
metal servers, you would have to manage all of these outside the cluster.

348
00:22:15,156 --> 00:22:19,448
So wouldn't it be great if we had a simple solution

349
00:22:19,544 --> 00:22:22,812
to all of this stuff as a part

350
00:22:22,866 --> 00:22:26,396
of the Kubernetes cluster itself, which means we

351
00:22:26,418 --> 00:22:30,372
would be able to configure the secure connection

352
00:22:30,456 --> 00:22:34,252
to the application as well as load balancing

353
00:22:34,396 --> 00:22:38,224
to all the different services in our cluster using

354
00:22:38,342 --> 00:22:41,984
solution as part of the cluster. That would be a

355
00:22:42,022 --> 00:22:45,776
much more elegant solution than having to create this load

356
00:22:45,808 --> 00:22:49,680
balancer components and then configure routing

357
00:22:49,760 --> 00:22:53,940
and all this stuff yourself. Well, the solution for all of this is

358
00:22:54,010 --> 00:22:57,700
actually ingress, which is part of the Kubernetes

359
00:22:57,780 --> 00:23:00,836
itself, and it lets you configure

360
00:23:00,948 --> 00:23:04,936
routing to different services and applications in

361
00:23:04,958 --> 00:23:09,512
your cluster using simple Kubernetes native

362
00:23:09,656 --> 00:23:13,404
components, as well as configure secure connection to

363
00:23:13,442 --> 00:23:17,212
any of your applications. This means that for our

364
00:23:17,266 --> 00:23:20,676
NgINX service application, we would simply

365
00:23:20,728 --> 00:23:23,948
need an ingress component in the cluster

366
00:23:24,044 --> 00:23:27,056
that will take the request and route that to the

367
00:23:27,078 --> 00:23:31,440
NgINX service. However, as we said, ingress is

368
00:23:31,590 --> 00:23:35,008
deployed and available inside the Kubernetes cluster.

369
00:23:35,104 --> 00:23:38,532
So to make it externally accessible we would still

370
00:23:38,586 --> 00:23:42,020
have to expose it either through a node port

371
00:23:42,090 --> 00:23:45,524
service or a load balancer. However, this would be much

372
00:23:45,562 --> 00:23:49,204
easier to do because we would just need one single node

373
00:23:49,252 --> 00:23:52,804
port or load balancer. That would be the single entry

374
00:23:52,852 --> 00:23:56,520
point to the cluster, which will then direct

375
00:23:56,590 --> 00:24:00,284
the traffic directly to ingress. And ingress will

376
00:24:00,322 --> 00:24:04,296
then manage all types of routing of the request

377
00:24:04,408 --> 00:24:08,252
within the cluster. So let's see how Ingress actually works

378
00:24:08,386 --> 00:24:12,224
and how it manages all this routing in an

379
00:24:12,262 --> 00:24:15,904
elegant and easy way so that we don't have to do any

380
00:24:15,942 --> 00:24:18,400
of this route configuration ourselves.

381
00:24:20,500 --> 00:24:23,244
So let's go through the syntax of Ingress.

382
00:24:23,372 --> 00:24:26,768
Basically you have a kind ingress instead of a service,

383
00:24:26,934 --> 00:24:30,644
and in a specification where the whole configuration happens

384
00:24:30,762 --> 00:24:34,112
you have so called rules or routing rules,

385
00:24:34,256 --> 00:24:38,216
and this basically defines that the main address or all

386
00:24:38,238 --> 00:24:42,292
the requests to that host must be forwarded

387
00:24:42,356 --> 00:24:45,784
to an internal service. So this is the

388
00:24:45,822 --> 00:24:49,176
host that user will enter in the browser. And in

389
00:24:49,198 --> 00:24:52,780
ingress users define a mapping. So what happens when that

390
00:24:52,850 --> 00:24:56,216
request to that host gets issued? You redirect

391
00:24:56,248 --> 00:25:00,504
it internally to a service. The path here basically means the URL

392
00:25:00,552 --> 00:25:04,060
path, so everything after the domain name,

393
00:25:04,130 --> 00:25:08,192
so slash whatever path comes after that. You can define those

394
00:25:08,246 --> 00:25:11,692
rules here and we'll see some different examples

395
00:25:11,756 --> 00:25:16,048
of the path configuration later. And as you see here in this configuration

396
00:25:16,224 --> 00:25:19,684
we have an HTTP protocol. So later

397
00:25:19,802 --> 00:25:23,360
in this video I'm going to show you how to configure htps

398
00:25:23,520 --> 00:25:26,064
connection using ingress component.

399
00:25:26,192 --> 00:25:30,456
So right now in the specification we don't have anything configured for

400
00:25:30,478 --> 00:25:34,196
the secure connection, it's just HTP. And one thing to note

401
00:25:34,228 --> 00:25:38,244
here is that this HTTP attribute here does not correspond

402
00:25:38,372 --> 00:25:41,932
to this one here. This is a protocol that the

403
00:25:41,986 --> 00:25:45,896
incoming request gets forwarded to to the internal

404
00:25:45,928 --> 00:25:49,676
service. So this is actually the second step and not

405
00:25:49,698 --> 00:25:53,664
to confuse it with this one. And now let's see how

406
00:25:53,782 --> 00:25:56,896
the internal service to that ingress will look like.

407
00:25:56,998 --> 00:26:00,752
So basically backend is the target where

408
00:26:00,806 --> 00:26:04,256
the request, the incoming request will be redirected and the

409
00:26:04,278 --> 00:26:07,844
service name should correspond the internal service name like this,

410
00:26:07,962 --> 00:26:11,716
and the port should be the internal service port.

411
00:26:11,818 --> 00:26:15,364
And as you see here, the only difference between the external and

412
00:26:15,402 --> 00:26:18,788
internal services is that here in internal service I don't

413
00:26:18,804 --> 00:26:21,864
have the third port which is the node port starting from

414
00:26:21,902 --> 00:26:25,144
30,000. We now have that attribute here, and the

415
00:26:25,182 --> 00:26:29,284
type is a default type, not a load balancer, but internal

416
00:26:29,332 --> 00:26:33,084
service type, which is cluster IP. So this should be a

417
00:26:33,122 --> 00:26:36,812
valid domain address. So you can just write anything here.

418
00:26:36,866 --> 00:26:40,492
It has to be first of all valid. And you should map that

419
00:26:40,546 --> 00:26:44,064
domain name to IP address of the

420
00:26:44,102 --> 00:26:47,612
node that represents an entry point to your Kubernetes

421
00:26:47,676 --> 00:26:51,660
cluster. If you configure a server outside of the Kubernetes cluster,

422
00:26:51,740 --> 00:26:55,456
that will become the entry point to your Kubernetes cluster.

423
00:26:55,568 --> 00:26:59,140
Then you should map this host name to the ip address

424
00:26:59,210 --> 00:27:02,912
of that server. So now that we saw what Kubernetes

425
00:27:02,976 --> 00:27:06,340
ingress components looks like, let's see how to actually configure

426
00:27:06,420 --> 00:27:10,452
ingress in the cluster. So basically you have a pod

427
00:27:10,516 --> 00:27:14,440
service and corresponding ingress. Now if you

428
00:27:14,510 --> 00:27:17,332
create that ingress component alone,

429
00:27:17,476 --> 00:27:21,576
that won't be enough for ingress routing

430
00:27:21,608 --> 00:27:25,528
rules to work. What you need in addition is an implementation

431
00:27:25,624 --> 00:27:29,656
for ingress, and that implementation is called ingress controller.

432
00:27:29,768 --> 00:27:33,344
So the step one will be to install an ingress controller which

433
00:27:33,382 --> 00:27:36,752
is basically another pod or another set

434
00:27:36,806 --> 00:27:40,252
of pods that run on your node in your Kubernetes

435
00:27:40,316 --> 00:27:44,400
cluster and does evaluation and processing of ingress

436
00:27:44,480 --> 00:27:48,944
rules. So the YamL file that I showed you with the ingress

437
00:27:49,072 --> 00:27:52,356
component is basically this part right here,

438
00:27:52,458 --> 00:27:56,112
and this has to be additionally installed in Kubernetes

439
00:27:56,176 --> 00:27:59,240
cluster. So what is ingress controller exactly?

440
00:27:59,390 --> 00:28:02,728
The function of ingress controller is to

441
00:28:02,814 --> 00:28:06,584
evaluate all the rules that you have defined in your cluster and this

442
00:28:06,622 --> 00:28:10,252
way to manage all the redirections. So basically this

443
00:28:10,306 --> 00:28:13,916
will be the entry point in the cluster for all the

444
00:28:14,018 --> 00:28:17,356
requests to that domain or subdomain rules that

445
00:28:17,378 --> 00:28:20,588
you've configured. And this will evaluate all the

446
00:28:20,594 --> 00:28:24,108
rules because you may have 50 rules or 50 ingress

447
00:28:24,284 --> 00:28:27,756
components created in your cluster, it will evaluate

448
00:28:27,788 --> 00:28:30,988
all the rules and decide based on that which forwarding

449
00:28:31,084 --> 00:28:34,008
rule applies for that specific request.

450
00:28:34,124 --> 00:28:37,876
So in order to install this implementation of ingress in

451
00:28:37,898 --> 00:28:41,428
your cluster, you have to decide which one of

452
00:28:41,514 --> 00:28:45,108
many different third party implementations you want to choose from.

453
00:28:45,194 --> 00:28:48,820
There is one from Kubernetes itself, which is Kubernetes NgInx

454
00:28:48,900 --> 00:28:53,172
Ingress controller, but there are others as well. So once you install ingress

455
00:28:53,236 --> 00:28:56,520
controller in your cluster, you're good to go create

456
00:28:56,590 --> 00:29:00,104
Ingress roles and the whole configuration is going to work.

457
00:29:00,222 --> 00:29:04,232
So now that I've shown you how Ingress can be used in a Kubernetes

458
00:29:04,296 --> 00:29:07,500
cluster, there is one thing that I think is important

459
00:29:07,570 --> 00:29:10,732
to understand in terms of setting up the whole

460
00:29:10,786 --> 00:29:13,692
cluster to be able to receive external requests.

461
00:29:13,836 --> 00:29:17,008
Now first of all, you have to consider

462
00:29:17,094 --> 00:29:20,816
the environment where your Kubernetes cluster is running. If you are

463
00:29:20,838 --> 00:29:24,656
using some cloud service provider like Amazon

464
00:29:24,688 --> 00:29:28,772
web services, Google Cloud, Linode. There are a couple more that

465
00:29:28,826 --> 00:29:32,612
have out of the box Kubernetes solutions or

466
00:29:32,666 --> 00:29:35,868
they have their own virtualized load balancer etc.

467
00:29:35,984 --> 00:29:39,576
Your cluster configuration would look something like this.

468
00:29:39,678 --> 00:29:43,620
So you would have a cloud load balancer that is specifically

469
00:29:43,700 --> 00:29:47,844
implemented by that cloud provider and external

470
00:29:47,892 --> 00:29:51,656
requests coming from the browser will first hit the load

471
00:29:51,688 --> 00:29:55,240
balancer and that load balancer then will redirect

472
00:29:55,320 --> 00:29:58,684
the request to ingress controller. Now this is

473
00:29:58,722 --> 00:30:01,720
not the only way to do it even in cloud environment.

474
00:30:01,800 --> 00:30:05,136
You can do it in a couple of different ways, but this is one of

475
00:30:05,158 --> 00:30:08,764
the most common strategies. An advantage

476
00:30:08,892 --> 00:30:12,176
of using cloud provider for that is that you

477
00:30:12,198 --> 00:30:16,068
don't have to implement a load balancer yourself.

478
00:30:16,234 --> 00:30:19,828
So with minimal effort, probably on most

479
00:30:19,914 --> 00:30:23,860
cloud providers, you will have the load balancer up and running and

480
00:30:23,930 --> 00:30:27,496
ready to receive those requests and forward those requests into your

481
00:30:27,518 --> 00:30:31,428
Kubernetes cluster. So very easy setup. Now if you're deploying

482
00:30:31,444 --> 00:30:35,304
your Kubernetes cluster on a bare metal environment, then you would

483
00:30:35,342 --> 00:30:39,368
have to do that part yourself. So basically you would have to configure

484
00:30:39,464 --> 00:30:43,356
some kind of entry point to your Kubernetes cluster yourself, either inside of

485
00:30:43,378 --> 00:30:47,660
a cluster or outside. As a separate server

486
00:30:48,240 --> 00:30:52,764
you will have to provide an entry point and one of those types

487
00:30:52,812 --> 00:30:56,384
is an external proxy server which can be a

488
00:30:56,422 --> 00:31:00,064
software or hardware solution that

489
00:31:00,182 --> 00:31:03,324
will take a role of that load balancer in an entry

490
00:31:03,372 --> 00:31:06,836
point to your cluster. So basically what this would

491
00:31:06,858 --> 00:31:10,836
mean is that you will have a separate server and

492
00:31:10,858 --> 00:31:14,768
you would give this a public ip address and you would open the ports

493
00:31:14,864 --> 00:31:18,228
in order for the requests to be accepted.

494
00:31:18,404 --> 00:31:22,324
And this proxy server then will act as an entry

495
00:31:22,372 --> 00:31:25,544
point to your cluster. So this will be the only one

496
00:31:25,662 --> 00:31:29,576
accessible externally. So none of the servers in your Kubernetes cluster

497
00:31:29,608 --> 00:31:33,756
will have publicly accessible IP address, which is obviously

498
00:31:33,858 --> 00:31:37,496
a very good security practice. So all the requests

499
00:31:37,528 --> 00:31:41,424
will enter the proxy server and that will then redirect the request to

500
00:31:41,462 --> 00:31:45,420
ingress controller and ingress controller will then decide

501
00:31:45,500 --> 00:31:49,660
which ingress rule applies to that specific request

502
00:31:49,820 --> 00:31:53,410
and the whole internal request forwarding will happen.

503
00:31:56,760 --> 00:32:00,656
In the next section we're going to go through more use cases

504
00:32:00,768 --> 00:32:05,040
of how you can define more fine granular routing

505
00:32:05,120 --> 00:32:08,872
for applications inside Kubernetes cluster. So the first

506
00:32:08,926 --> 00:32:12,536
thing is defining multiple path of

507
00:32:12,558 --> 00:32:16,520
the same host. So consider following use case.

508
00:32:16,670 --> 00:32:20,824
Google has one domain but has many services that

509
00:32:20,862 --> 00:32:24,556
it offers. So for example if you have a Google account, you can use

510
00:32:24,578 --> 00:32:28,648
its analytics, you can use its shopping, you have a calendar,

511
00:32:28,744 --> 00:32:33,116
you have a Gmail, et cetera. So all of these are separate applications

512
00:32:33,308 --> 00:32:36,268
that are accessible with the same domain.

513
00:32:36,364 --> 00:32:40,512
So consider you have an application that does something similar.

514
00:32:40,646 --> 00:32:44,224
So you offer two separate applications that are

515
00:32:44,262 --> 00:32:48,164
part of the same ecosystem, but you still want to have them

516
00:32:48,282 --> 00:32:51,664
on separate URLs. So what you can do is that in rules

517
00:32:51,712 --> 00:32:55,428
you can define the host, which is myapp.com,

518
00:32:55,594 --> 00:32:59,664
and in the path section you can define multiple path. So if

519
00:32:59,722 --> 00:33:03,128
user wants to access your analytics application,

520
00:33:03,294 --> 00:33:07,076
then they have to enter myapp.com analytics

521
00:33:07,188 --> 00:33:10,616
and that will forward the request to internal

522
00:33:10,728 --> 00:33:14,204
analytics service and the pod. Or if they want

523
00:33:14,242 --> 00:33:18,056
to access the shopping application, then the URL

524
00:33:18,088 --> 00:33:21,144
for that would be myapp.com shopping.

525
00:33:21,272 --> 00:33:24,652
So this way you can do forwarding with one ingress

526
00:33:24,796 --> 00:33:28,924
of the same host to multiple applications using multiple

527
00:33:28,972 --> 00:33:32,800
path. Another use case is when instead

528
00:33:32,870 --> 00:33:36,660
of using URLs to make

529
00:33:36,730 --> 00:33:40,484
different applications accessible, some companies use

530
00:33:40,602 --> 00:33:45,056
subdomains. So instead of having myapp.com analytics,

531
00:33:45,168 --> 00:33:49,832
they create a subdomain analytics myapp.com.

532
00:33:49,966 --> 00:33:54,164
So if you have your application configured that way, your configuration

533
00:33:54,212 --> 00:33:58,264
will look like this. So instead of having one host like in the previous example,

534
00:33:58,382 --> 00:34:01,772
and multiple path here inside, now you have

535
00:34:01,826 --> 00:34:05,640
multiple hosts where each host represents a subdomain

536
00:34:05,720 --> 00:34:09,496
and inside you just have one path that again redirects

537
00:34:09,528 --> 00:34:13,116
that request to analytics service. Pretty straightforward.

538
00:34:13,228 --> 00:34:17,584
So now in the same request setting you have analytics service and

539
00:34:17,782 --> 00:34:21,248
a pod behind it. Now the request will look like

540
00:34:21,414 --> 00:34:24,696
this using a subdomain instead of path.

541
00:34:24,828 --> 00:34:28,132
And one final topic that I mentioned that we'll cover

542
00:34:28,186 --> 00:34:31,200
here is configuring TLS certificate.

543
00:34:31,280 --> 00:34:34,880
Till now we've only seen ingress configuration for HTP

544
00:34:34,960 --> 00:34:39,172
requests, but it's super easy to configure HTTPs

545
00:34:39,316 --> 00:34:43,192
forwarding in Ingress. So the only thing that you need to do is

546
00:34:43,326 --> 00:34:46,964
define attribute called TLS above the rules

547
00:34:47,012 --> 00:34:50,196
section with host which is the same host as right

548
00:34:50,238 --> 00:34:54,600
here, and the secret name which is a reference

549
00:34:54,680 --> 00:34:58,700
of a secret that you have to create in a cluster that holds that

550
00:34:58,770 --> 00:35:02,304
TLS certificate. So the secret configuration would look

551
00:35:02,342 --> 00:35:05,360
like this. So the name is

552
00:35:05,510 --> 00:35:08,576
the reference right here, and the data or

553
00:35:08,598 --> 00:35:12,768
the actual contents contain TLS certificate and

554
00:35:12,854 --> 00:35:16,832
TLS key. You probably notice the type additional

555
00:35:16,896 --> 00:35:21,012
type attribute here. In Kubernetes there is a specific

556
00:35:21,146 --> 00:35:25,284
type of a secret called TLS, so we'll have to use that

557
00:35:25,322 --> 00:35:28,504
type when you create a TLS secret. And there are

558
00:35:28,542 --> 00:35:32,328
three small notes to be made here. One is that

559
00:35:32,414 --> 00:35:35,800
the keys of this data have to be named exactly

560
00:35:35,870 --> 00:35:39,336
like that. The values are the actual

561
00:35:39,438 --> 00:35:42,844
file contents of the certificate or key contents and not

562
00:35:42,882 --> 00:35:46,316
the file path or location. So you have to put the

563
00:35:46,338 --> 00:35:49,932
whole content here, base 64 encoded. And the third

564
00:35:49,986 --> 00:35:54,124
one is that you have to create the secret in the same namespace

565
00:35:54,252 --> 00:35:58,528
as the ingress component for it to be able to use that.

566
00:35:58,694 --> 00:36:02,364
Otherwise you can't reference a secret from another namespace.

567
00:36:02,492 --> 00:36:05,664
And these four lines is all you need to

568
00:36:05,702 --> 00:36:08,976
configure mapping of an HTTPs request

569
00:36:09,088 --> 00:36:12,230
to that host to internal service.

570
00:36:19,630 --> 00:36:22,906
We know what Ingress is, how it works, and how we

571
00:36:22,928 --> 00:36:26,462
can configure ingress in our cluster. Let's actually

572
00:36:26,516 --> 00:36:29,674
go ahead and set it up. As you learned,

573
00:36:29,722 --> 00:36:33,550
the first step to using ingress and configuring it is to

574
00:36:33,620 --> 00:36:36,754
actually install the ingress controller application,

575
00:36:36,952 --> 00:36:40,382
the application that actually does routing

576
00:36:40,446 --> 00:36:44,142
in the cluster. And then after we've installed the Ingress

577
00:36:44,206 --> 00:36:48,294
controller, we're going to create ingress components in Kubernetes using

578
00:36:48,412 --> 00:36:52,070
Kubernetes native manifest files to

579
00:36:52,140 --> 00:36:55,574
configure our ingress controller application with

580
00:36:55,612 --> 00:36:59,430
the logic of how to route different application

581
00:36:59,580 --> 00:37:03,350
requests. So as a first step, let's install ingress

582
00:37:03,430 --> 00:37:07,322
controller in our cluster and we're going to install it using

583
00:37:07,456 --> 00:37:11,542
Helm. Helm is basically a package manager for Kubernetes

584
00:37:11,686 --> 00:37:15,534
that lets us install different applications with

585
00:37:15,572 --> 00:37:18,974
all the needed components inside. So it basically makes

586
00:37:19,012 --> 00:37:22,938
it easier for us to install applications in Kubernetes.

587
00:37:23,034 --> 00:37:26,466
If you don't know what Helm is and how it works, you can watch the

588
00:37:26,488 --> 00:37:30,306
prerequisite video about Helm and then continue with the

589
00:37:30,328 --> 00:37:34,610
lecture where we install ingress controller manager with helm.

590
00:37:38,960 --> 00:37:42,840
In this video I'm going to explain all the main concepts of helm

591
00:37:42,920 --> 00:37:46,796
so that you are able to use it in your own projects. So understanding

592
00:37:46,828 --> 00:37:50,192
the basic common principles and more importantly its

593
00:37:50,246 --> 00:37:53,628
use cases to when and why we use Helm

594
00:37:53,724 --> 00:37:57,444
will make it easier for you to use it in practice, no matter which

595
00:37:57,482 --> 00:38:00,628
version you choose. So the topics I'm going to go through in

596
00:38:00,634 --> 00:38:04,308
this video are Helm and helm charts, what they are,

597
00:38:04,394 --> 00:38:07,690
how to use them, and in which scenarios they're used.

598
00:38:08,620 --> 00:38:12,724
So what is Helm? Helm has a couple of main features

599
00:38:12,852 --> 00:38:16,404
that it's used for. The first one is as a package

600
00:38:16,452 --> 00:38:19,788
manager for Kubernetes, so you can think of it as

601
00:38:19,954 --> 00:38:23,096
apt Yum or homebrew for Kubernetes.

602
00:38:23,288 --> 00:38:26,876
So it's a convenient way for packaging collections of

603
00:38:26,898 --> 00:38:30,784
Kubernetes YaML files and distributing them in public and

604
00:38:30,822 --> 00:38:34,368
private registry. Now these definitions may sound a

605
00:38:34,374 --> 00:38:38,560
bit abstract, so let's break them down with specific examples.

606
00:38:39,140 --> 00:38:42,548
So let's say you have deployed your application in

607
00:38:42,554 --> 00:38:46,608
Kubernetes cluster and you want to deploy elasticsearch

608
00:38:46,704 --> 00:38:50,404
additionally in your cluster that your application will use

609
00:38:50,522 --> 00:38:54,324
to collect its logs. In order to

610
00:38:54,442 --> 00:38:58,600
deploy Elasticstack in your Kubernetes cluster,

611
00:38:59,340 --> 00:39:02,776
you would need a couple of Kubernetes components. So you would need a

612
00:39:02,798 --> 00:39:06,024
stateful set which is for stateful applications like

613
00:39:06,062 --> 00:39:10,600
databases, you would need a config map with external configuration

614
00:39:10,680 --> 00:39:14,344
you would need a secret where some credentials and secret

615
00:39:14,392 --> 00:39:18,204
data are stored. You would need to create the Kubernetes user with its

616
00:39:18,322 --> 00:39:21,776
respective permissions and also create a couple of

617
00:39:21,798 --> 00:39:25,712
services. Now if you were to create

618
00:39:25,766 --> 00:39:29,664
all of these files manually by searching for each

619
00:39:29,702 --> 00:39:33,516
one of them separately on Internet, be a tedious job

620
00:39:33,638 --> 00:39:37,456
and until you have all these YAML files collected and tested

621
00:39:37,488 --> 00:39:41,024
and try it out, it might take some time. And since elasticstack

622
00:39:41,072 --> 00:39:44,724
deployment is pretty much the standard across all

623
00:39:44,762 --> 00:39:48,056
clusters, other people will probably have to go through the

624
00:39:48,078 --> 00:39:51,924
same. So it made perfect sense that someone created

625
00:39:51,972 --> 00:39:55,896
these YaMl files once and packaged them up and made

626
00:39:55,918 --> 00:39:59,916
it available somewhere so that other people who

627
00:39:59,938 --> 00:40:03,464
also use the same kind of deployment could use them in their kubernetes

628
00:40:03,512 --> 00:40:07,532
cluster. And that bundle of YAML files is

629
00:40:07,586 --> 00:40:11,184
called Helm chart. So using helm you can create

630
00:40:11,222 --> 00:40:14,496
your own helm charts or bundles of those

631
00:40:14,598 --> 00:40:18,576
YAMl files and push them to some

632
00:40:18,758 --> 00:40:22,412
helm repository to make it available for others.

633
00:40:22,566 --> 00:40:26,052
Or you can consume so you can download and use

634
00:40:26,186 --> 00:40:29,604
existing helm charts that other people pushed and made

635
00:40:29,642 --> 00:40:33,252
available in different repositories. So commonly used

636
00:40:33,306 --> 00:40:36,788
deployments like database applications, elasticsearch,

637
00:40:36,884 --> 00:40:40,276
MongoDB, MySQL, or monitoring applications

638
00:40:40,308 --> 00:40:43,528
like Prometheus that all have this kind

639
00:40:43,534 --> 00:40:46,852
of complex setup, all have charts

640
00:40:46,916 --> 00:40:50,812
available in some helm repository. So using

641
00:40:50,866 --> 00:40:54,568
a simple helm install chart name command,

642
00:40:54,664 --> 00:40:58,284
you can reuse the configuration that someone else has already made

643
00:40:58,402 --> 00:41:01,792
without additional effort. And sometimes that someone is even

644
00:41:01,846 --> 00:41:05,872
the company that created the application and this functionality of

645
00:41:05,926 --> 00:41:09,808
sharing charts that became pretty widely used

646
00:41:09,894 --> 00:41:13,036
actually was one of the contributors to why Helm

647
00:41:13,068 --> 00:41:16,112
became so popular compared to its alternative

648
00:41:16,176 --> 00:41:19,488
tools. So now if you have a cluster

649
00:41:19,584 --> 00:41:22,996
and you need some kind of deployment that you think

650
00:41:23,098 --> 00:41:26,840
should be available out there, you can actually look it up, either using

651
00:41:26,910 --> 00:41:30,244
command line so you can do helm search with a keyword,

652
00:41:30,372 --> 00:41:34,232
or you can go to either Helm's own public

653
00:41:34,286 --> 00:41:37,672
repository, helm hub, or on helm charts

654
00:41:37,736 --> 00:41:42,028
pages or other repositories that are available now.

655
00:41:42,114 --> 00:41:45,464
Apart from those public registries for Helm

656
00:41:45,512 --> 00:41:49,228
charts, they're also private registries. Because when companies

657
00:41:49,314 --> 00:41:53,296
started creating those charts, they also started distributing them

658
00:41:53,398 --> 00:41:56,960
among or internally in the organization. So it made perfect

659
00:41:57,030 --> 00:42:00,624
sense to create registries to share those

660
00:42:00,742 --> 00:42:04,144
charts within the organization and not publicly.

661
00:42:04,272 --> 00:42:08,800
So there are a couple of tools out there that are used as helm

662
00:42:08,880 --> 00:42:12,832
chart private repositories as well. Another functionality

663
00:42:12,896 --> 00:42:16,472
of helm is that it's a templating engine.

664
00:42:16,606 --> 00:42:20,024
So what does that actually mean? Imagine you have an

665
00:42:20,062 --> 00:42:24,724
application that is made up of multiple microservices

666
00:42:24,852 --> 00:42:27,944
and you're deploying all of them in your Kubernetes cluster.

667
00:42:28,072 --> 00:42:32,232
And deployment and service of each of those microservices

668
00:42:32,376 --> 00:42:35,516
are pretty much the same, with the only difference that

669
00:42:35,538 --> 00:42:38,860
the application name and version are different or

670
00:42:38,930 --> 00:42:42,832
the docker image name and version tags are different.

671
00:42:42,966 --> 00:42:46,492
So without helm you would write separate

672
00:42:46,556 --> 00:42:50,092
YAML files, configuration files for each of those microservices.

673
00:42:50,156 --> 00:42:53,508
So you would have multiple deployment service

674
00:42:53,594 --> 00:42:57,924
files where each one has its own application

675
00:42:58,042 --> 00:43:01,908
name and version defined. But since the only difference

676
00:43:01,994 --> 00:43:05,808
between those YAML files are just couple of lines

677
00:43:05,904 --> 00:43:09,496
or a couple of values using helm, what you can do is that

678
00:43:09,518 --> 00:43:13,480
you can define a common blueprint for all the microservices

679
00:43:13,820 --> 00:43:17,636
and the values that are dynamic, or the values that are going to

680
00:43:17,678 --> 00:43:21,816
change replace by placeholders.

681
00:43:22,008 --> 00:43:25,996
And that would be a template file. So the template file would

682
00:43:26,018 --> 00:43:29,180
look something like this. You would have a template file which is standard

683
00:43:29,250 --> 00:43:33,104
YAml, but instead of values in some places you would have the

684
00:43:33,142 --> 00:43:37,536
syntax, which means that you're taking a value from

685
00:43:37,638 --> 00:43:40,812
external configuration and that external configuration.

686
00:43:40,876 --> 00:43:45,152
If you see the syntax here values, that external configuration

687
00:43:45,216 --> 00:43:48,708
comes from an additional YAMl file which is called

688
00:43:48,794 --> 00:43:52,884
values Yaml. And here you can define all

689
00:43:52,922 --> 00:43:56,136
those values that you're going to use in

690
00:43:56,158 --> 00:44:00,132
that template file. So for example here those four values are defined

691
00:44:00,196 --> 00:44:03,592
in an values YAMl file and what

692
00:44:03,646 --> 00:44:07,880
dot values is. It's an object that is being created

693
00:44:08,240 --> 00:44:12,536
based on the values that are supplied via values YAMl file

694
00:44:12,648 --> 00:44:16,190
and also through command line using

695
00:44:17,280 --> 00:44:20,924
set flag. So whichever way you define those

696
00:44:20,962 --> 00:44:24,504
additional values, they're combined and put together in

697
00:44:24,642 --> 00:44:28,256
values object that you can then use in those template files to

698
00:44:28,278 --> 00:44:31,804
get the values out. So now instead of having YAML

699
00:44:31,852 --> 00:44:35,636
files for each microservice, you just have one and you

700
00:44:35,658 --> 00:44:39,264
can simply replace those values dynamically.

701
00:44:39,392 --> 00:44:43,136
And this is especially practical when you're using continuous

702
00:44:43,168 --> 00:44:47,096
delivery, continuous integration for your application, because what you can

703
00:44:47,118 --> 00:44:51,204
do is that in your build pipeline you can use those template

704
00:44:51,252 --> 00:44:55,176
YAMl files and replace the values on

705
00:44:55,198 --> 00:44:58,876
the fly before deploying them. Another use case where

706
00:44:58,898 --> 00:45:02,380
you can use the helm features of

707
00:45:02,450 --> 00:45:06,092
package manager and templating engine is when you

708
00:45:06,226 --> 00:45:10,332
deploy the same set of applications across different Kubernetes

709
00:45:10,396 --> 00:45:14,192
clusters. So consider a use case where you have your

710
00:45:14,246 --> 00:45:18,236
microservice application that you want to deploy

711
00:45:18,268 --> 00:45:21,708
on development, staging and production clusters.

712
00:45:21,884 --> 00:45:25,956
So instead of deploying the individual YAML files separately in

713
00:45:25,978 --> 00:45:29,764
each cluster, you can package them up to make your

714
00:45:29,802 --> 00:45:33,188
own application chart that will have all the

715
00:45:33,274 --> 00:45:37,096
necessary YAML files that that particular

716
00:45:37,278 --> 00:45:41,192
deployment needs, and then you can use them to

717
00:45:41,326 --> 00:45:44,852
redeploy the same application in different Kubernetes

718
00:45:44,916 --> 00:45:48,424
cluster environments using one command, which can also

719
00:45:48,462 --> 00:45:52,236
make the whole deployment process easier. So now that you

720
00:45:52,258 --> 00:45:55,804
know what helm charts are used for it, let's actually look

721
00:45:55,842 --> 00:45:59,148
at an example helm chart structure to have

722
00:45:59,154 --> 00:46:02,736
a better understanding. So typically chart is made

723
00:46:02,758 --> 00:46:06,432
up of such a directory structure. So the top

724
00:46:06,486 --> 00:46:09,888
level will be the name of the chart and inside the directory you

725
00:46:09,894 --> 00:46:13,244
would have following. So chart Yaml

726
00:46:13,292 --> 00:46:17,440
is basically a file that contains all the matter information about the chart

727
00:46:17,520 --> 00:46:21,696
could be name and version, maybe list of dependencies et cetera.

728
00:46:21,888 --> 00:46:25,480
Values Yaml that I mentioned before is place

729
00:46:25,550 --> 00:46:29,208
where all the values are configured for

730
00:46:29,294 --> 00:46:32,744
the template files, and this will actually be the

731
00:46:32,782 --> 00:46:36,490
default values that you can override later.

732
00:46:36,860 --> 00:46:40,376
The charts directory will have chart dependencies

733
00:46:40,488 --> 00:46:44,364
inside, meaning that if this chart depends on

734
00:46:44,482 --> 00:46:47,804
other charts, then those chart dependencies will be

735
00:46:47,842 --> 00:46:51,376
stored here and templates folder is basically where the

736
00:46:51,398 --> 00:46:55,964
template files are stored. So when you execute

737
00:46:56,012 --> 00:46:59,724
helm install command to actually deploy those yaml

738
00:46:59,772 --> 00:47:03,664
files into kubernetes, the template files

739
00:47:03,712 --> 00:47:07,280
from here will be filled with the values from values

740
00:47:07,360 --> 00:47:10,976
Yaml, producing valid kubernetes manifests

741
00:47:11,008 --> 00:47:13,460
that can then be deployed into kubernetes.

742
00:47:14,360 --> 00:47:18,484
And optionally you can have some other files in this folder

743
00:47:18,612 --> 00:47:22,420
like readme or license file et cetera.

744
00:47:22,580 --> 00:47:26,372
So to have a better understanding of how values are injected

745
00:47:26,436 --> 00:47:30,072
into helm templates, consider that in values

746
00:47:30,136 --> 00:47:33,992
Yaml, which is a default value configuration, you have following

747
00:47:34,056 --> 00:47:37,272
three values, image name, port and version.

748
00:47:37,416 --> 00:47:41,324
And as I mentioned, the default values that are defined

749
00:47:41,372 --> 00:47:44,624
here can be overridden in a couple of different ways.

750
00:47:44,742 --> 00:47:48,528
One way is that when executing helm install

751
00:47:48,694 --> 00:47:52,760
command, you can provide an alternative values

752
00:47:52,860 --> 00:47:56,404
yaml file using values flag. So for

753
00:47:56,442 --> 00:48:00,304
example, if values yaml file will have following

754
00:48:00,352 --> 00:48:04,004
three values which are image name, port and version, you can define your

755
00:48:04,042 --> 00:48:07,948
own values yaml file called myvalues

756
00:48:08,064 --> 00:48:11,556
Yaml and you can override one of those values,

757
00:48:11,588 --> 00:48:15,032
or you can even add some new attributes there and

758
00:48:15,086 --> 00:48:18,536
those two will be merged which will result

759
00:48:18,638 --> 00:48:22,040
into a dot values object that will look like this.

760
00:48:22,110 --> 00:48:26,012
So we'd have image name and port from values Yaml and the one

761
00:48:26,066 --> 00:48:29,828
that you overwrote with your own values file.

762
00:48:29,944 --> 00:48:33,810
Alternatively, you can also provide additional individual

763
00:48:34,260 --> 00:48:37,536
values using set flag where you

764
00:48:37,558 --> 00:48:41,356
can define the values directly on the command line. But of course it's

765
00:48:41,388 --> 00:48:44,564
more organized and better manageable to

766
00:48:44,602 --> 00:48:47,968
have files where you store all those values instead of just providing

767
00:48:47,984 --> 00:48:49,270
them on the command line.

768
00:48:52,680 --> 00:48:56,664
Now that you know what helm is and that we're going to use it in

769
00:48:56,702 --> 00:49:00,116
the cluster to install applications,

770
00:49:00,228 --> 00:49:04,244
let's actually install helm itself so that we have helm

771
00:49:04,292 --> 00:49:07,808
command available. Right now we don't have it. And to install helm

772
00:49:07,844 --> 00:49:11,752
we can just check out their official documentation about installing Helm.

773
00:49:11,896 --> 00:49:15,752
Let's search for Ubuntu because that's the server

774
00:49:15,896 --> 00:49:19,404
operating system we're working on and we have two ways

775
00:49:19,442 --> 00:49:22,780
of installing it with apt package manager and snap.

776
00:49:22,860 --> 00:49:26,224
Let's actually go with the apt. I'm going to copy all of these

777
00:49:26,262 --> 00:49:29,410
commands and execute it right here.

778
00:49:31,380 --> 00:49:35,072
And apt get install helm. Let's execute.

779
00:49:35,216 --> 00:49:38,944
And if I do helm now you see that we have the command

780
00:49:38,992 --> 00:49:42,240
available with a version

781
00:49:42,400 --> 00:49:45,972
three six three. Awesome. So we already

782
00:49:46,026 --> 00:49:49,540
have helm installed on the master node,

783
00:49:49,620 --> 00:49:53,060
which means we can use it to install Nginx Ingress

784
00:49:53,140 --> 00:49:56,964
controller in our cluster. There is one thing I want to note about helm,

785
00:49:57,012 --> 00:50:00,952
which is we don't have to explicitly connect helm

786
00:50:01,016 --> 00:50:04,764
command to the cluster. We can simply go ahead and

787
00:50:04,802 --> 00:50:08,156
start executing helm install commands. So you may

788
00:50:08,178 --> 00:50:11,596
be wondering how does helm then know in which cluster

789
00:50:11,708 --> 00:50:15,472
it should install applications inside or how to even connect

790
00:50:15,526 --> 00:50:18,976
to any cluster? Well, in the background helm will

791
00:50:18,998 --> 00:50:23,056
actually use the kubeconfig file that is configured

792
00:50:23,168 --> 00:50:26,836
by default for Kubectl. So the config file in

793
00:50:26,858 --> 00:50:31,584
the Linux user's home directory Kubefolder,

794
00:50:31,632 --> 00:50:35,432
which Kubectl uses to connect to the cluster is also

795
00:50:35,486 --> 00:50:38,516
used by helm command to connect to the cluster.

796
00:50:38,628 --> 00:50:42,730
So whatever Kubectl command is pointing to

797
00:50:43,980 --> 00:50:47,736
will be cluster which helm will connect to. And that is

798
00:50:47,758 --> 00:50:51,310
very convenient because we don't have to do any extra work here.

799
00:51:25,060 --> 00:51:28,912
Awesome. So how do we install a package or application using

800
00:51:28,966 --> 00:51:32,244
helm? Well, we find a helm church that we need. In our

801
00:51:32,282 --> 00:51:36,116
case, this is the ingress Nginx controller that

802
00:51:36,138 --> 00:51:39,556
we want to install and it is actually in the

803
00:51:39,578 --> 00:51:43,044
Kubernetes repository itself. And right here we

804
00:51:43,082 --> 00:51:46,916
have the commands for installing it. So I'm just going to copy

805
00:51:46,948 --> 00:51:51,160
this. Helm Repo Ed will basically just add

806
00:51:51,310 --> 00:51:54,824
a repository where the chart is located and then

807
00:51:54,862 --> 00:51:58,316
it will update the repository. And now we

808
00:51:58,338 --> 00:52:02,028
can install the chart which again we have command for it

809
00:52:02,114 --> 00:52:03,070
right here.

810
00:52:07,600 --> 00:52:11,468
Let's call it Ingress Nginx.

811
00:52:11,644 --> 00:52:15,164
That's the name that you can come up with and give your chart

812
00:52:15,212 --> 00:52:18,732
installation. And this is the repository

813
00:52:18,796 --> 00:52:22,272
name and chart name. So execute.

814
00:52:22,416 --> 00:52:25,780
And we have a nice output that says

815
00:52:25,930 --> 00:52:30,400
chart deployed as well as an example ingress

816
00:52:30,480 --> 00:52:34,024
configuration file right here. So for creating the

817
00:52:34,062 --> 00:52:37,640
first ingress component, you could just copy this

818
00:52:37,710 --> 00:52:41,204
here and get started with it. If I do helm

819
00:52:41,332 --> 00:52:46,640
ls, I will see the chart information namespace

820
00:52:46,740 --> 00:52:50,792
in which the application and all the components

821
00:52:50,936 --> 00:52:54,604
related to it were created, as well as the

822
00:52:54,642 --> 00:52:58,140
chart version and the application version.

823
00:52:58,800 --> 00:53:02,140
And that means we should have new pods running

824
00:53:02,210 --> 00:53:06,064
in a cluster in default namespace. So if I do

825
00:53:06,102 --> 00:53:10,172
kubectl get pod, you see that we have ingress

826
00:53:10,316 --> 00:53:13,476
Nginx controller pod, one replica of

827
00:53:13,498 --> 00:53:15,700
it running in our cluster.

828
00:53:17,640 --> 00:53:21,316
And if I do get service, you see that we

829
00:53:21,338 --> 00:53:25,156
have two services in the cluster. One of them is internal

830
00:53:25,188 --> 00:53:29,060
service and another one is load balancer.

831
00:53:29,220 --> 00:53:33,130
Remember when I mentioned that Ingress application

832
00:53:33,740 --> 00:53:37,036
also needs to be exposed in some way to

833
00:53:37,058 --> 00:53:40,700
the external traffic, either using a node port service

834
00:53:40,770 --> 00:53:44,984
or a load balancer service. And this is exactly the load balancer

835
00:53:45,032 --> 00:53:48,520
service that ingress controller

836
00:53:48,680 --> 00:53:52,876
needs to be accessible from external applications.

837
00:53:52,988 --> 00:53:56,524
And as we learned in a setup like this, load balancer

838
00:53:56,572 --> 00:53:59,520
does not automatically get created by the platform.

839
00:53:59,670 --> 00:54:03,504
So we would have to actually configure it just like we configured

840
00:54:03,552 --> 00:54:07,008
a load balancer for our Nginx service. So the way it's

841
00:54:07,024 --> 00:54:10,564
going to work is we're going to create a load balancer that

842
00:54:10,602 --> 00:54:14,868
will forward the requests to ingress controller

843
00:54:15,044 --> 00:54:18,456
on whatever node port it's available on.

844
00:54:18,638 --> 00:54:22,744
And right here you see the port or node port on

845
00:54:22,782 --> 00:54:26,404
which ingress Nginx controller application is

846
00:54:26,462 --> 00:54:29,336
available. On each of the worker nodes,

847
00:54:29,448 --> 00:54:32,776
there are actually two node ports where the controller

848
00:54:32,808 --> 00:54:36,296
is available. One of them is for HTTP traffic,

849
00:54:36,408 --> 00:54:40,340
that's port 80. Another one is for HTPS

850
00:54:40,520 --> 00:54:44,368
traffic, which is four, four, three. That means

851
00:54:44,454 --> 00:54:48,048
we're going to configure a load balancer with this information.

852
00:54:48,214 --> 00:54:51,856
And this is going to be easy since you have already configured a load

853
00:54:51,888 --> 00:54:55,556
balancer for another service. So let's go ahead and do

854
00:54:55,578 --> 00:54:56,950
that as a next step.

855
00:54:59,800 --> 00:55:06,680
So we can actually get rid of this load balancer here and

856
00:55:06,750 --> 00:55:10,724
create a new one for HTP HTPS traffic.

857
00:55:10,852 --> 00:55:14,760
And let's call it Ingress NginX

858
00:55:16,160 --> 00:55:20,856
controller load balancer. Again, Internet facing,

859
00:55:21,048 --> 00:55:24,716
and this is the load balancer port. So load balancer itself

860
00:55:24,818 --> 00:55:27,932
will be accessible on port 80, which is the default one.

861
00:55:28,066 --> 00:55:32,048
So users don't have to explicitly type it, which is fine.

862
00:55:32,134 --> 00:55:35,932
We're going to leave it at that. And again choose two subnets,

863
00:55:35,996 --> 00:55:38,770
one of them where our work nodes are running.

864
00:55:39,700 --> 00:55:43,908
Next, choose the default security group

865
00:55:44,074 --> 00:55:47,524
and create a new target group. Let's call

866
00:55:47,562 --> 00:55:51,460
it Ingress target group, instance type and

867
00:55:51,530 --> 00:55:54,520
port for HTP traffic,

868
00:55:55,180 --> 00:56:00,312
which as we saw here is 32,675.

869
00:56:00,366 --> 00:56:05,272
And

870
00:56:05,326 --> 00:56:08,604
let's register the two worker nodes that we

871
00:56:08,642 --> 00:56:14,740
have and

872
00:56:14,890 --> 00:56:16,580
create the load balancer.

873
00:56:18,600 --> 00:56:22,048
In our case, again, because this is not an AWS

874
00:56:22,144 --> 00:56:25,780
specific setup, we're just going to do a simple configuration

875
00:56:25,940 --> 00:56:29,524
just for the HTP traffic. So our load balancer

876
00:56:29,572 --> 00:56:33,284
is active, which means using the load balancer's

877
00:56:33,332 --> 00:56:36,600
IP address or the domain name right here,

878
00:56:36,750 --> 00:56:39,736
we can now access the cluster.

879
00:56:39,928 --> 00:56:43,692
So right now, because we don't have anything configured, if I just

880
00:56:43,746 --> 00:56:47,116
type it in the browser and execute, we're going to get a

881
00:56:47,138 --> 00:56:50,272
gateway timeout error, which means we were able

882
00:56:50,326 --> 00:56:53,456
to access the ingress controller. However,

883
00:56:53,638 --> 00:56:56,924
the ingress controller doesn't know what to do with that request

884
00:56:56,972 --> 00:57:00,416
because we haven't configured anything. So it just gives us a

885
00:57:00,438 --> 00:57:04,484
gateway timeout error. But that means we're connecting to

886
00:57:04,522 --> 00:57:08,036
the cluster through NginX controller. And as

887
00:57:08,058 --> 00:57:12,244
a next step we're going to configure ingress controller with

888
00:57:12,362 --> 00:57:15,992
logic of how to route different requests to

889
00:57:16,046 --> 00:57:19,460
our applications. And for that we're going to create an ingress

890
00:57:19,540 --> 00:57:20,360
component.

891
00:57:32,630 --> 00:57:36,222
So think of ingress components as

892
00:57:36,376 --> 00:57:40,054
a configuration piece that we

893
00:57:40,092 --> 00:57:43,954
basically pass on to the ingress NginX controller

894
00:57:44,002 --> 00:57:47,414
application. Telling here is what to do when this

895
00:57:47,452 --> 00:57:51,334
kind of request comes in. So with every ingress component

896
00:57:51,382 --> 00:57:54,746
we create in the cluster, we basically hand over a

897
00:57:54,768 --> 00:57:58,234
new configuration logic to our

898
00:57:58,352 --> 00:58:01,786
ingress controller application. So that's a Kubernetes

899
00:58:01,898 --> 00:58:05,194
native way, by configuring routing logic

900
00:58:05,242 --> 00:58:09,514
in the ingress controller application. So what do we want that routing

901
00:58:09,642 --> 00:58:13,434
logic to be in our case? Well, we want Ingress to

902
00:58:13,492 --> 00:58:16,994
forward any request coming inside the

903
00:58:17,032 --> 00:58:20,866
cluster to NgInX service internal service.

904
00:58:21,048 --> 00:58:24,626
That's what we want. So that's the rule that we're going to configure in

905
00:58:24,648 --> 00:58:28,242
our ingress controller application using the ingress

906
00:58:28,386 --> 00:58:31,874
component. So before that I'm actually going to delete the NgInx

907
00:58:31,922 --> 00:58:45,370
service load balancer and

908
00:58:45,440 --> 00:58:51,854
adjust our service configuration file to

909
00:58:51,892 --> 00:58:54,560
be a cluster IP type,

910
00:58:55,650 --> 00:59:02,366
which means this is only accessible internally and

911
00:59:02,388 --> 00:59:03,680
let's apply it again.

912
00:59:10,230 --> 00:59:13,378
And now we have an internal NgINX service

913
00:59:13,544 --> 00:59:16,854
and basically all the services from now on will

914
00:59:16,892 --> 00:59:20,646
be internal because we have an entry point in the

915
00:59:20,668 --> 00:59:24,614
cluster which is Ingress controller, and all the traffic will

916
00:59:24,652 --> 00:59:28,694
be first accepted by Ingress controller. And Ingress

917
00:59:28,742 --> 00:59:32,662
controller will then decide based on the logic, we configure

918
00:59:32,806 --> 00:59:36,906
which internal service it's going to send that request to.

919
00:59:37,008 --> 00:59:40,970
So forwarding a traffic received on HTP port to

920
00:59:41,040 --> 00:59:45,114
NgInx service, that's what we want to configure. So let's create an

921
00:59:45,152 --> 00:59:47,530
ingress configuration file.

922
00:59:50,750 --> 00:59:54,498
For that, I'm actually going to use the create

923
00:59:54,584 --> 00:59:58,306
Ingress imperative command and then using

924
00:59:58,408 --> 00:59:59,760
dry run option.

