1
00:00:00,330 --> 00:00:03,870
So we have the private and public key pair, but we want the public

2
00:00:03,940 --> 00:00:07,402
key to be certified by a trusted

3
00:00:07,466 --> 00:00:10,686
CA so that clients will accept it. CA will then

4
00:00:10,708 --> 00:00:13,646
actually certify the details of your website,

5
00:00:13,828 --> 00:00:17,566
company, domain name, ownership, etc. All the data that

6
00:00:17,588 --> 00:00:21,006
you entered and if everything is fine they

7
00:00:21,028 --> 00:00:24,606
will issue a signed certificate and send it back to you. So if

8
00:00:24,628 --> 00:00:28,726
the hack try to get the certificate signed by the CA they

9
00:00:28,748 --> 00:00:32,322
will fail in the information validation stage and won't

10
00:00:32,386 --> 00:00:35,798
get the certificate from the CA. So that's how the whole process

11
00:00:35,884 --> 00:00:39,354
works. And this whole infrastructure with the process of

12
00:00:39,472 --> 00:00:43,274
issuing, validating, maintaining certificates as well as

13
00:00:43,312 --> 00:00:47,082
all the CAS involved servers, keys, et cetera is called

14
00:00:47,136 --> 00:00:51,870
a public key infrastructure or PKI.

15
00:01:04,020 --> 00:01:07,824
Now that we have the infrastructure ready, it's time to get started and set

16
00:01:07,862 --> 00:01:12,060
up a Kubernetes cluster from scratch, which is super exciting.

17
00:01:12,220 --> 00:01:15,876
But how does it all work? What applications do

18
00:01:15,898 --> 00:01:18,836
you need to install and how do you install them?

19
00:01:19,018 --> 00:01:22,868
In this section we will go through the whole setup and how everything

20
00:01:22,954 --> 00:01:26,212
works in detail. First we need to deploy

21
00:01:26,276 --> 00:01:29,880
a container runtime on every node so that

22
00:01:29,950 --> 00:01:33,816
containers can be scheduled on them. After that we

23
00:01:33,838 --> 00:01:37,460
need to install Kubelet application also on every node

24
00:01:37,540 --> 00:01:40,700
which will basically run as a regular Linux process

25
00:01:40,770 --> 00:01:43,900
just like the container runtime. So they will both

26
00:01:43,970 --> 00:01:48,204
be installed from a package repository just like we install any

27
00:01:48,242 --> 00:01:50,988
application on Linux servers.

28
00:01:51,164 --> 00:01:55,148
And again these two need to run on each server

29
00:01:55,244 --> 00:01:58,400
whether it's a master node or worker node.

30
00:01:58,740 --> 00:02:02,884
And with these two in place we can now deploy pods for

31
00:02:02,922 --> 00:02:06,452
all the other Kubernetes components on the

32
00:02:06,506 --> 00:02:10,512
control plane node. We need to now deploy pods

33
00:02:10,656 --> 00:02:14,624
which run the master applications. So API,

34
00:02:14,672 --> 00:02:18,188
server, scheduler, controller, manager and eTCD

35
00:02:18,304 --> 00:02:22,212
applications will all be deployed as pods

36
00:02:22,356 --> 00:02:25,764
on the master node. Plus we shouldn't forget

37
00:02:25,812 --> 00:02:29,372
about the kubeproxy application which again will be deployed on

38
00:02:29,426 --> 00:02:32,764
each node because worker nodes also need

39
00:02:32,802 --> 00:02:36,348
them and kubeproxy application will also run

40
00:02:36,434 --> 00:02:39,692
as a pod. Now there are two things

41
00:02:39,746 --> 00:02:42,908
we need in order to deploy these applications.

42
00:02:43,084 --> 00:02:47,468
The first one is that since all these applications are pods,

43
00:02:47,564 --> 00:02:50,672
we need the Kubernetes manifest file for

44
00:02:50,726 --> 00:02:53,876
each application. And the second thing is that we

45
00:02:53,898 --> 00:02:57,968
need to make sure these applications are also deployed securely.

46
00:02:58,064 --> 00:03:01,556
This means unauthorized users shouldn't be

47
00:03:01,578 --> 00:03:06,052
able to access these applications and also the applications

48
00:03:06,116 --> 00:03:10,090
should talk to each other securely in an encrypted way.

49
00:03:10,460 --> 00:03:14,570
So let's see both of these points in more detail.

50
00:03:17,700 --> 00:03:21,364
First of all, as I mentioned, all master components will be

51
00:03:21,402 --> 00:03:24,580
deployed as pods. But as you already know,

52
00:03:24,730 --> 00:03:28,356
we need the master components in order to deploy the

53
00:03:28,378 --> 00:03:31,816
pods, because when deploying a pod, we need

54
00:03:31,838 --> 00:03:35,396
to send a request to the API server. The scheduler

55
00:03:35,428 --> 00:03:39,268
component will then decide where the pod should be scheduled

56
00:03:39,364 --> 00:03:43,172
and also the pod data will be written

57
00:03:43,236 --> 00:03:46,600
into and updated in itSCD storage.

58
00:03:46,760 --> 00:03:50,200
So as you see, when scheduling one pod,

59
00:03:50,280 --> 00:03:53,756
all the master components are involved. So how do

60
00:03:53,778 --> 00:03:57,296
we schedule these components without

61
00:03:57,398 --> 00:04:00,924
having them in the cluster? So we have this chicken

62
00:04:00,972 --> 00:04:04,204
and the egg problem. Well, for that specifically,

63
00:04:04,252 --> 00:04:07,596
we have what's called static pods.

64
00:04:07,788 --> 00:04:11,636
Static pods are basically just like any other pod, but they

65
00:04:11,658 --> 00:04:14,868
are directly scheduled by the Kubelet without

66
00:04:14,954 --> 00:04:18,532
the need for API server or itsyd or

67
00:04:18,586 --> 00:04:22,232
scheduler. So the request for scheduling that

68
00:04:22,286 --> 00:04:26,420
pod doesn't go through the API server, it goes directly

69
00:04:26,500 --> 00:04:30,532
to Kubelet. So once we have the container runtime and Kubelet

70
00:04:30,596 --> 00:04:34,092
running, we can actually schedule these

71
00:04:34,146 --> 00:04:37,596
static pods. So generally with

72
00:04:37,618 --> 00:04:40,488
all the regular pods, as you already learned,

73
00:04:40,584 --> 00:04:44,440
API server will get the request first to schedule

74
00:04:44,520 --> 00:04:48,176
a pod. So scheduler will decide which node it

75
00:04:48,198 --> 00:04:52,064
should be scheduled on and then API server will contact the

76
00:04:52,102 --> 00:04:55,196
kubelet on that selected node

77
00:04:55,388 --> 00:04:59,452
that scheduler selected and tell Kubelet

78
00:04:59,516 --> 00:05:03,712
on that node to schedule the pod. So that's a way of scheduling

79
00:05:03,776 --> 00:05:07,204
regular pods. But in addition to getting these commands from

80
00:05:07,242 --> 00:05:10,432
the API server, Kubelet can actually schedule

81
00:05:10,496 --> 00:05:13,752
pods on its own. How does it happen?

82
00:05:13,886 --> 00:05:17,464
Well, Kubelet continuously watches a

83
00:05:17,502 --> 00:05:21,144
specific location on the node where

84
00:05:21,182 --> 00:05:28,096
it's running. And that location is etcmanifests

85
00:05:28,228 --> 00:05:32,264
and it watches that folder for any Kubernetes manifest

86
00:05:32,312 --> 00:05:35,756
files. And if it finds a pod manifest there,

87
00:05:35,858 --> 00:05:39,580
it will schedule it as a static pod. No master

88
00:05:39,660 --> 00:05:42,912
processes required. Now why is it called

89
00:05:42,966 --> 00:05:46,652
a static pod and how is it different from the regular

90
00:05:46,716 --> 00:05:49,756
pods deployed through the API server?

91
00:05:49,948 --> 00:05:53,236
Well, there are a couple of differences. First of all,

92
00:05:53,338 --> 00:05:57,264
even though static pods are visible on the API server,

93
00:05:57,392 --> 00:06:01,200
they cannot be controlled from there. So it's a responsibility of

94
00:06:01,290 --> 00:06:05,000
Kubelet. So controller manager, for example, doesn't manage

95
00:06:05,070 --> 00:06:08,872
that pod. So if it crashes or doesn't work

96
00:06:08,926 --> 00:06:12,344
anymore, it's Kubelet's responsibility to

97
00:06:12,382 --> 00:06:16,124
pick that up and reschedule it again. And also

98
00:06:16,322 --> 00:06:19,624
you can easily identify the static pods

99
00:06:19,752 --> 00:06:23,084
by their names because at the end of their

100
00:06:23,202 --> 00:06:26,676
name they get a suffix of the node

101
00:06:26,728 --> 00:06:30,432
name they are running on. So this all means that

102
00:06:30,486 --> 00:06:34,172
when installing a Kubernetes cluster, we would need to generate

103
00:06:34,236 --> 00:06:37,548
the static pod manifests for the API server,

104
00:06:37,644 --> 00:06:42,064
controller, manager, scheduler and eTCD applications

105
00:06:42,192 --> 00:06:48,688
and put these manifest files into manifest

106
00:06:48,784 --> 00:06:52,324
folder where Kubelet will find them and schedule

107
00:06:52,372 --> 00:06:56,794
them when

108
00:06:56,912 --> 00:07:00,954
deploying these applications, we need to ensure that they

109
00:07:00,992 --> 00:07:04,266
will run securely. So the question is

110
00:07:04,368 --> 00:07:07,998
how do they talk to each other securely? How can each

111
00:07:08,084 --> 00:07:11,950
component identify the other component

112
00:07:12,290 --> 00:07:16,170
in the communication process and also establish a mutual

113
00:07:16,250 --> 00:07:19,330
TLS connection so that the communication between

114
00:07:19,400 --> 00:07:22,942
them is encrypted? Well, we need certificates

115
00:07:23,006 --> 00:07:26,834
for that. Now let's see exactly who talks to

116
00:07:26,872 --> 00:07:30,754
who in Kubernetes cluster and what certificates we need

117
00:07:30,792 --> 00:07:34,038
for them. Well, the API server is in the center

118
00:07:34,124 --> 00:07:37,190
of all the things and all the communication.

119
00:07:37,610 --> 00:07:41,314
So almost every other component will talk to the API

120
00:07:41,362 --> 00:07:44,902
server and that means every component that wants to

121
00:07:44,956 --> 00:07:48,746
talk to the API server will need to provide a certificate to

122
00:07:48,768 --> 00:07:52,550
be able to authenticate itself with the API server.

123
00:07:52,630 --> 00:07:56,538
So the way it works is we need to first generate a self signed

124
00:07:56,634 --> 00:08:00,382
CA certificate for kubernetes which we will then

125
00:08:00,436 --> 00:08:04,334
use to sign all the client and server certificates for

126
00:08:04,372 --> 00:08:08,430
each component in the cluster. And we can store these certificates

127
00:08:08,510 --> 00:08:12,302
in PKI

128
00:08:12,366 --> 00:08:15,554
folder. So API server will have a

129
00:08:15,592 --> 00:08:19,438
server certificate and scheduler and controller manager

130
00:08:19,534 --> 00:08:23,206
will both have their client certificates to talk to the

131
00:08:23,228 --> 00:08:27,206
API server. Same way API server talks to

132
00:08:27,308 --> 00:08:30,306
ETCD and Kubelet applications,

133
00:08:30,418 --> 00:08:34,822
which means ECD and Kubelet will need their own server certificates.

134
00:08:34,966 --> 00:08:38,406
And since in this case API server is the client,

135
00:08:38,518 --> 00:08:41,994
it will need its own client certificate to talk

136
00:08:42,032 --> 00:08:45,646
to the ETCD and Kubelet applications so

137
00:08:45,668 --> 00:08:49,182
that it can authenticate itself with them. For that

138
00:08:49,236 --> 00:08:52,554
it can use its existing server certificate.

139
00:08:52,682 --> 00:08:56,362
Or alternatively we should create new client

140
00:08:56,426 --> 00:09:00,322
certificates specifically for talking to

141
00:09:00,376 --> 00:09:04,110
iTSCD and Kubelet applications. Kubelet also talks

142
00:09:04,190 --> 00:09:07,394
back to the API server, so it can also use

143
00:09:07,432 --> 00:09:10,838
a separate client certificate for that. So basically

144
00:09:10,924 --> 00:09:14,274
the idea is that whenever an application talks

145
00:09:14,322 --> 00:09:17,062
to the API server, for example,

146
00:09:17,196 --> 00:09:20,886
API server doesn't know whether that application is

147
00:09:20,988 --> 00:09:24,314
a legitimate application, whether the request is coming

148
00:09:24,352 --> 00:09:27,994
from a hacker or an unauthorized source or whatever. So for

149
00:09:28,032 --> 00:09:31,514
API server, any application connecting to it

150
00:09:31,552 --> 00:09:34,102
is basically just a regular client.

151
00:09:34,246 --> 00:09:38,362
And for every such client, API server

152
00:09:38,506 --> 00:09:42,458
needs to make sure that this client

153
00:09:42,554 --> 00:09:46,078
is authorized. And the same goes for

154
00:09:46,244 --> 00:09:49,330
ETCD when API server connects to it. Right.

155
00:09:49,400 --> 00:09:54,190
ETCD has to decide is this client authorized

156
00:09:54,270 --> 00:09:58,530
to talk to me? And to ensure proper identification,

157
00:09:58,950 --> 00:10:02,274
each component will get a certificate

158
00:10:02,402 --> 00:10:06,066
signed by the same certificate authority.

159
00:10:06,258 --> 00:10:09,766
So now when scheduler connects to API server it

160
00:10:09,788 --> 00:10:13,674
will say this is my certificate which is a proof that

161
00:10:13,712 --> 00:10:16,378
I am part of the same cluster as you are.

162
00:10:16,464 --> 00:10:19,482
Therefore I should be allowed to talk to you.

163
00:10:19,616 --> 00:10:22,134
So that's how the whole setup,

164
00:10:22,262 --> 00:10:26,014
public key infrastructure PKI should

165
00:10:26,052 --> 00:10:30,014
work in Kubernetes. So all these components talk

166
00:10:30,052 --> 00:10:32,362
to each other to do their jobs.

167
00:10:32,506 --> 00:10:36,174
But there is one more client that we need to

168
00:10:36,212 --> 00:10:39,506
give certificate or access to the cluster to

169
00:10:39,608 --> 00:10:42,974
and that is ourselves as administrators

170
00:10:43,022 --> 00:10:46,914
of the cluster. Right, because we as admins also need to

171
00:10:46,952 --> 00:10:50,466
talk to the API server to administer it because as

172
00:10:50,488 --> 00:10:53,634
you learned, all the queries and updates in the cluster

173
00:10:53,762 --> 00:10:57,590
go through the API server. And this means we also

174
00:10:57,660 --> 00:11:01,538
need our own client certificate for the admin user

175
00:11:01,634 --> 00:11:05,178
to authenticate with the API server. And of course

176
00:11:05,264 --> 00:11:09,510
to be valid and accepted by API server. This certificate

177
00:11:09,590 --> 00:11:13,334
also needs to be signed by the CA that we created

178
00:11:13,382 --> 00:11:17,574
in Kubernetes. So when preparing the installation

179
00:11:17,622 --> 00:11:21,678
of the cluster, all these certificates need to be created first

180
00:11:21,764 --> 00:11:25,726
the self signed CA certificate for the whole cluster and

181
00:11:25,748 --> 00:11:29,426
then all the server and client certificates which will be

182
00:11:29,528 --> 00:11:32,610
signed by the CA certificate that we created.

183
00:11:36,090 --> 00:11:39,526
So these are the main things that we need

184
00:11:39,548 --> 00:11:43,722
to prepare for the cluster. Plus there are a couple of minor things

185
00:11:43,776 --> 00:11:47,914
and configuration details we also need to provide

186
00:11:48,112 --> 00:11:51,706
to install Kubernetes cluster. So as you

187
00:11:51,728 --> 00:11:55,166
see, all of this is pretty complex and it

188
00:11:55,188 --> 00:11:58,650
would be really difficult to set all this up manually

189
00:11:58,810 --> 00:12:02,122
like generating all these certificates

190
00:12:02,186 --> 00:12:05,914
and making them available for each pod, creating those manifest

191
00:12:05,962 --> 00:12:09,870
files and configurations for the applications

192
00:12:10,030 --> 00:12:13,970
and so on. Well, thankfully there is a command line tool

193
00:12:14,040 --> 00:12:17,714
called Kubeadm or Kubeadmin which

194
00:12:17,752 --> 00:12:21,222
will bootstrap all of this and do all this stuff and

195
00:12:21,276 --> 00:12:25,202
generate all the necessary configurations and certificates

196
00:12:25,266 --> 00:12:28,914
and everything in the background. And Kubeadm

197
00:12:28,962 --> 00:12:31,810
is created and maintained by Kubernetes itself.

198
00:12:31,980 --> 00:12:36,442
So in the Kubernetes official documentation, Kubeadm is

199
00:12:36,576 --> 00:12:40,506
used as one way of installing the cluster and you have all

200
00:12:40,528 --> 00:12:44,474
the documentation of Kubeadm and its commands here and

201
00:12:44,512 --> 00:12:47,870
that's the tool we're going to use to create our cluster.

202
00:12:53,450 --> 00:12:57,046
Great, so now we know what we need to do to install the

203
00:12:57,068 --> 00:13:00,934
cluster, so let's actually go ahead and do it. So you go

204
00:13:00,972 --> 00:13:04,474
ahead and open the Kubernetes documentation to work through it

205
00:13:04,512 --> 00:13:08,202
and use it as a guide. So once infrastructure is

206
00:13:08,256 --> 00:13:12,138
provisioned, we need to check our prerequisites and configure a

207
00:13:12,144 --> 00:13:16,090
couple of things on those servers so that Kubernetes

208
00:13:16,170 --> 00:13:19,610
can work properly on them after installation.

209
00:13:19,770 --> 00:13:23,470
And these are our prerequisites. We already checked this before

210
00:13:23,540 --> 00:13:27,230
creating the infrastructure to make sure we have the required

211
00:13:27,310 --> 00:13:31,854
resources on our servers. And we have a couple of more prerequisite

212
00:13:31,902 --> 00:13:35,774
things here. The first two are already satisfied. We have unique

213
00:13:35,822 --> 00:13:39,766
host names on our AWS infrastructure as

214
00:13:39,788 --> 00:13:42,966
well as our servers are in the same

215
00:13:43,068 --> 00:13:46,534
private network in the VPC. So that

216
00:13:46,572 --> 00:13:50,022
should be fine, but we need to disable the swap and

217
00:13:50,076 --> 00:13:53,302
also open some ports on our machines.

218
00:13:53,446 --> 00:13:56,714
So let's go ahead and do those two things. The first

219
00:13:56,752 --> 00:14:00,198
one is pretty easy to disable swap on our server.

220
00:14:00,374 --> 00:14:03,840
We execute swap of

221
00:14:05,170 --> 00:14:08,734
a command and we need to do that on each of

222
00:14:08,772 --> 00:14:12,960
the servers, so we're going to do that on worker nodes as well later.

223
00:14:16,130 --> 00:14:20,238
And the second thing is about opening ports

224
00:14:20,334 --> 00:14:23,666
on our machine. So if I click here, these are basically a

225
00:14:23,688 --> 00:14:27,202
list of ports that we need to configure on control

226
00:14:27,256 --> 00:14:30,806
plane nodes and on worker nodes. Now what are

227
00:14:30,828 --> 00:14:34,662
all these ports and why do we have to configure them right now?

228
00:14:34,716 --> 00:14:38,598
If we check our AWS infrastructure and

229
00:14:38,684 --> 00:14:42,426
security, for example, you see that we only allow

230
00:14:42,608 --> 00:14:46,506
incoming traffic to our servers on port

231
00:14:46,608 --> 00:14:50,602
22. So that's the only port that is open on our

232
00:14:50,656 --> 00:14:54,218
server. And this is same for all the nodes.

233
00:14:54,314 --> 00:14:58,574
So every node has the same security configuration where

234
00:14:58,612 --> 00:15:02,526
we just allow incoming traffic on port 22

235
00:15:02,628 --> 00:15:05,830
so that we are able to ssh into those servers.

236
00:15:05,930 --> 00:15:09,566
And that's a default config on AWS infrastructure.

237
00:15:09,678 --> 00:15:13,506
This means all the other components that are running on

238
00:15:13,528 --> 00:15:17,278
those servers but on different ports are not accessible

239
00:15:17,374 --> 00:15:20,754
from another instance when we install the cluster.

240
00:15:20,802 --> 00:15:24,306
However, the applications on master nodes and worker

241
00:15:24,338 --> 00:15:26,690
nodes like API server,

242
00:15:26,850 --> 00:15:30,422
scheduler, etCD, Kubelet and so on

243
00:15:30,556 --> 00:15:34,394
need to be able to talk to each other on different ports because

244
00:15:34,432 --> 00:15:38,234
as we learned, these applications communicate with each other to

245
00:15:38,272 --> 00:15:41,702
execute certain tasks like API server

246
00:15:41,766 --> 00:15:45,054
talks to Kubelet and HCD and all other services

247
00:15:45,172 --> 00:15:48,494
talk to the Kube API. And without opening those

248
00:15:48,532 --> 00:15:51,546
ports we wouldn't even be able to join worker

249
00:15:51,578 --> 00:15:55,530
nodes to the cluster or add more master nodes.

250
00:15:55,610 --> 00:15:58,766
So let's configure all this on our AWS,

251
00:15:58,878 --> 00:16:02,866
which is actually pretty easy because we can do this on

252
00:16:02,888 --> 00:16:06,562
the UI using the security groups of our

253
00:16:06,616 --> 00:16:10,214
servers. So again, in the security part of my server I

254
00:16:10,252 --> 00:16:13,814
see which security group it is using and if I click

255
00:16:13,852 --> 00:16:17,874
inside I can actually edit the incoming traffic

256
00:16:17,922 --> 00:16:21,414
or inbound traffic rules of that

257
00:16:21,452 --> 00:16:24,278
security group that will then apply to the server.

258
00:16:24,374 --> 00:16:28,582
So let's click on it and use this security group to configure

259
00:16:28,726 --> 00:16:32,138
rules for the control plane node. So let's go

260
00:16:32,144 --> 00:16:37,342
ahead and open those ports. The first port we have is 6443,

261
00:16:37,476 --> 00:16:41,114
which is the port where by default

262
00:16:41,242 --> 00:16:43,902
Kubernetes API server is running.

263
00:16:44,036 --> 00:16:47,794
And all the clients, whether they are inside the

264
00:16:47,832 --> 00:16:51,602
cluster or outside the cluster, need to be able to connect

265
00:16:51,656 --> 00:16:55,474
to it. So the source is basically every IP address.

266
00:16:55,592 --> 00:16:59,894
So let's copy this port. Let's add the rule that's the port

267
00:17:00,092 --> 00:17:04,070
number. And as I said, every IP address

268
00:17:04,220 --> 00:17:08,358
should be able to access it because API server is

269
00:17:08,524 --> 00:17:12,790
the entry point to the cluster and when we execute Kubectl commands

270
00:17:12,870 --> 00:17:16,438
we want to be able to connect to it from outside the cluster.

271
00:17:16,534 --> 00:17:20,314
So that's one rule. Let's check out other ones. The second

272
00:17:20,352 --> 00:17:24,254
one is a range, a port range from

273
00:17:24,372 --> 00:17:27,934
2379 to 80. So these

274
00:17:27,972 --> 00:17:31,646
two ports are used by itSCD server and we need

275
00:17:31,668 --> 00:17:35,786
to open them as well. However, Etsy server doesn't

276
00:17:35,818 --> 00:17:39,714
need to be accessible from outside the cluster because only

277
00:17:39,912 --> 00:17:44,174
the Kube API server or the ECD itself is connecting

278
00:17:44,222 --> 00:17:47,858
to it. So we don't want to open access to this port

279
00:17:47,944 --> 00:17:51,574
from everywhere, but only from inside

280
00:17:51,692 --> 00:17:55,762
the cluster. So how do we know what is the IP address range

281
00:17:55,826 --> 00:17:59,250
of the instances which are part of this cluster,

282
00:17:59,330 --> 00:18:02,746
so only they need access to it. And remember, in security there

283
00:18:02,768 --> 00:18:06,854
is a best practice that says that you always give the minimum permission

284
00:18:06,902 --> 00:18:10,266
required to do the task. So we only want

285
00:18:10,368 --> 00:18:14,010
these ports to be accessible from within this

286
00:18:14,080 --> 00:18:17,434
private network. And to find the IP address range

287
00:18:17,482 --> 00:18:21,662
of the private network, as you already learned in the prerequisite lecture of

288
00:18:21,716 --> 00:18:25,934
AWS, we go to the VPC where all our servers

289
00:18:25,982 --> 00:18:29,422
are running. So that's the VPC and our VPC

290
00:18:29,486 --> 00:18:32,942
has this IP address range.

291
00:18:33,086 --> 00:18:37,422
So basically that means that any instance which is created

292
00:18:37,486 --> 00:18:41,266
inside that VPC, any server will have an IP

293
00:18:41,298 --> 00:18:44,710
address from this range and we want those

294
00:18:44,780 --> 00:18:48,790
instances to be able to access this port,

295
00:18:48,940 --> 00:18:52,874
so only accessible within that private network. And the

296
00:18:52,912 --> 00:18:56,486
same will be true for these three ports

297
00:18:56,518 --> 00:19:00,714
here, 10,200 and 5251 52,

298
00:19:00,832 --> 00:19:04,034
which are respectively used by Kubelet

299
00:19:04,182 --> 00:19:08,346
application, the scheduler and controller manager. So let's

300
00:19:08,378 --> 00:19:11,886
actually copy the first one and let's make it a range as

301
00:19:11,908 --> 00:19:15,450
well. So 250 to 252,

302
00:19:15,620 --> 00:19:19,378
that's the range. And again, we only allow it

303
00:19:19,464 --> 00:19:23,154
within that VPC private network that

304
00:19:23,192 --> 00:19:26,866
basically configures the ports and access

305
00:19:27,048 --> 00:19:30,582
to these ports on

306
00:19:30,636 --> 00:19:33,974
any server which uses this security group.

307
00:19:34,092 --> 00:19:37,970
And to make it more visible for us, the launch

308
00:19:38,050 --> 00:19:41,546
wizard two. This is the security group that we

309
00:19:41,648 --> 00:19:45,194
just edited. Let's actually give it a name as well.

310
00:19:45,232 --> 00:19:48,922
And let's call it Master security Group.

311
00:19:49,056 --> 00:19:53,294
And going back in the master node and

312
00:19:53,332 --> 00:19:53,920
security,

313
00:19:56,450 --> 00:20:00,266
we see the adjusted security rules or firewall

314
00:20:00,298 --> 00:20:03,722
rules that are configured for the master

315
00:20:03,786 --> 00:20:07,154
node. So master node port configuration is done.

316
00:20:07,272 --> 00:20:10,830
Let's now go and configure the same for worker nodes.

317
00:20:10,910 --> 00:20:15,102
And clicking on worker node one, you see that it uses

318
00:20:15,246 --> 00:20:18,914
a different security group, which worker node

319
00:20:19,042 --> 00:20:22,806
two and one both share. So they use the same security group,

320
00:20:22,908 --> 00:20:26,840
which we're going to now adjust and configure with

321
00:20:28,330 --> 00:20:31,574
new rules. So let's open that edit

322
00:20:31,622 --> 00:20:35,290
inbound rules. And the first rule is

323
00:20:35,360 --> 00:20:38,886
again, Kubelet, which is running on master

324
00:20:38,998 --> 00:20:42,982
and worker nodes. And Kubelet needs to be open because

325
00:20:43,136 --> 00:20:46,574
Kube API server from master Node will talk

326
00:20:46,612 --> 00:20:49,966
to the Kubelet on each worker node. So that's why we

327
00:20:49,988 --> 00:20:54,000
need to make it accessible. So let's copy that port

328
00:20:54,610 --> 00:20:58,466
is 10,250 and

329
00:20:58,568 --> 00:21:02,290
again, only from inside the VPC

330
00:21:02,870 --> 00:21:06,434
because no external connection to

331
00:21:06,472 --> 00:21:10,450
Kubelet is required. So we don't have to give anyone else permission

332
00:21:10,530 --> 00:21:14,482
to talk to it. And the second rule, port range,

333
00:21:14,626 --> 00:21:17,942
which is for node port type

334
00:21:18,076 --> 00:21:22,342
services. So basically whenever we configure, create a

335
00:21:22,476 --> 00:21:26,902
node port service, it will make the application accessible

336
00:21:26,966 --> 00:21:30,346
on one of these ports. And obviously to access the

337
00:21:30,368 --> 00:21:33,790
application on this port, the port needs to be open.

338
00:21:33,940 --> 00:21:37,594
So we're going to copy that and edit

339
00:21:37,642 --> 00:21:41,534
as a range and as a source. We're going to choose

340
00:21:41,652 --> 00:21:44,942
everyone because these ports are basically where

341
00:21:44,996 --> 00:21:48,334
applications will run. So it should be accessible

342
00:21:48,382 --> 00:21:52,034
from browser. So any external client will

343
00:21:52,072 --> 00:21:55,842
be able to access it. And let's save this

344
00:21:55,976 --> 00:21:59,894
again, I'm going to copy the name and

345
00:21:59,932 --> 00:22:04,070
filter it here. And let's call it workersg.

346
00:22:06,330 --> 00:22:10,258
So we have the master security group and worker security group

347
00:22:10,444 --> 00:22:15,018
with different port

348
00:22:15,104 --> 00:22:18,538
configurations. And again, just to make sure,

349
00:22:18,704 --> 00:22:22,698
let's go to worker node one. And there you

350
00:22:22,704 --> 00:22:30,156
go. We have the new rules here. And worker

351
00:22:30,188 --> 00:22:34,640
node two. There you go. So this takes care of the

352
00:22:34,710 --> 00:22:38,592
last prerequisite for opening the required ports

353
00:22:38,656 --> 00:22:41,300
so the cluster can function properly.

354
00:22:44,120 --> 00:22:48,260
And one last thing we're going to do in order to just

355
00:22:48,330 --> 00:22:51,784
optimize our cluster is give our

356
00:22:51,822 --> 00:22:55,432
hosts or our servers a more human readable name. So instead

357
00:22:55,486 --> 00:22:59,144
of the private ip address as a name of the

358
00:22:59,182 --> 00:23:03,476
host, we're going to call master node, master host,

359
00:23:03,588 --> 00:23:07,752
worker one and worker two. So right

360
00:23:07,806 --> 00:23:11,880
now I'm on the master node, so I'm going to connect to

361
00:23:12,030 --> 00:23:13,850
the worker nodes as well,

362
00:23:14,620 --> 00:23:40,726
and the

363
00:23:40,748 --> 00:23:42,190
second worker node.

364
00:23:45,490 --> 00:23:48,554
And there you go. So I'm connected to the master node,

365
00:23:48,682 --> 00:23:52,522
worker one, worker two. However, just from the names

366
00:23:52,586 --> 00:23:55,838
here, I don't know in my terminal

367
00:23:56,014 --> 00:23:59,922
which node is master, which is worker one, and so on. So when

368
00:23:59,976 --> 00:24:02,846
working and administering your cluster,

369
00:24:02,958 --> 00:24:07,054
it's actually very helpful to have that name, the host name appear

370
00:24:07,102 --> 00:24:10,446
right here so that you don't have to go back here and

371
00:24:10,488 --> 00:24:13,906
check basically which node you're on. Right. So that's

372
00:24:13,938 --> 00:24:17,474
what we're going to optimize now. We're going to give these hosts proper names

373
00:24:17,522 --> 00:24:21,018
and that's actually very easy to do. Let's clear this up. We're going

374
00:24:21,024 --> 00:24:26,970
to create entries for the host names in etc

375
00:24:28,030 --> 00:24:31,290
hosts folder and we're going to do that using pseudo.

376
00:24:32,610 --> 00:24:36,046
And at the end of the

377
00:24:36,068 --> 00:24:39,962
file we're going to add an entry or mapping

378
00:24:40,026 --> 00:24:43,774
for IP address, the internal private IP address of

379
00:24:43,812 --> 00:24:47,218
each node and to the name that we want to give it.

380
00:24:47,304 --> 00:24:50,882
So going back again the private IP address

381
00:24:51,016 --> 00:24:54,642
of the node, not the public one because that's how

382
00:24:54,696 --> 00:24:58,340
the instances or the servers talk to each other

383
00:24:58,790 --> 00:25:02,546
within VPC using the private IP address. So copy

384
00:25:02,578 --> 00:25:06,326
that. And this is

385
00:25:06,348 --> 00:25:10,470
going to be the master because that's how we decided.

386
00:25:10,630 --> 00:25:12,860
Then we have worker node one,

387
00:25:14,910 --> 00:25:18,220
let's call it worker one.

388
00:25:19,630 --> 00:25:26,702
And then we have worker two like

389
00:25:26,756 --> 00:25:31,178
this. And we need these three entries

390
00:25:31,354 --> 00:25:35,578
on each node inside this etc

391
00:25:35,754 --> 00:25:44,270
hosts file beam

392
00:25:46,050 --> 00:25:47,080
and it.

393
00:26:03,630 --> 00:26:07,094
So basically what these entries will do on each node

394
00:26:07,142 --> 00:26:10,570
is that whenever the master node wants to talk to

395
00:26:10,720 --> 00:26:14,126
one of these worker nodes, or vice versa, they can

396
00:26:14,148 --> 00:26:17,866
use the names instead of using the ip

397
00:26:17,898 --> 00:26:21,994
address to communicate with each other. So if master

398
00:26:22,042 --> 00:26:24,882
node says let me talk to worker one,

399
00:26:25,016 --> 00:26:28,654
we'll be able to get the ip address behind that worker

400
00:26:28,702 --> 00:26:32,942
one to connect to it. So those host

401
00:26:33,006 --> 00:26:37,014
name to ip address mapping is present on

402
00:26:37,052 --> 00:26:40,646
all the machines. What we also need to do is assign the

403
00:26:40,668 --> 00:26:44,166
host names to those servers. So we

404
00:26:44,188 --> 00:26:48,198
do that using pseudo host name

405
00:26:48,364 --> 00:26:52,810
control. It's a command set hostname

406
00:26:55,070 --> 00:26:58,826
and whatever hostname we want to give it, we're calling it

407
00:26:58,848 --> 00:27:00,860
master. There you go.

408
00:27:05,370 --> 00:27:10,694
We're calling this one worker one and

409
00:27:10,812 --> 00:27:12,920
this one worker two.

410
00:27:13,610 --> 00:27:17,174
And we don't see any changes here. And that's because

411
00:27:17,212 --> 00:27:20,650
we need to log out from the server and then log in again

412
00:27:20,720 --> 00:27:25,082
so that it will take effect. So we're going to do exit and

413
00:27:25,216 --> 00:27:29,206
connect again. And as you see we have master here

414
00:27:29,328 --> 00:27:33,258
and let's do the same here. Reconnect worker

415
00:27:33,274 --> 00:27:37,486
one, exit, reconnect and

416
00:27:37,588 --> 00:27:40,974
worker two. So now it's way easier for us

417
00:27:41,012 --> 00:27:44,850
to administer the cluster because we know immediately which

418
00:27:45,000 --> 00:28:06,284
server or which node we are on provisioned

419
00:28:06,332 --> 00:28:09,504
and configured. And we also made

420
00:28:09,542 --> 00:28:13,444
sure that all the prerequisites for installing the

421
00:28:13,482 --> 00:28:17,984
cluster are fulfilled. So we are ready to start installing

422
00:28:18,112 --> 00:28:21,844
the applications necessary to set up a

423
00:28:21,882 --> 00:28:25,428
cluster. And the first one is a container

424
00:28:25,524 --> 00:28:29,032
runtime. That's the first application that we're going to install

425
00:28:29,166 --> 00:28:32,276
and container runtime is not a Kubernetes

426
00:28:32,388 --> 00:28:36,312
component. It's basically a separate application that kubernetes

427
00:28:36,376 --> 00:28:39,752
will then use to schedule the containers.

428
00:28:39,896 --> 00:28:43,452
However, before we actually install a container runtime, let's actually

429
00:28:43,506 --> 00:28:47,020
understand what a container runtime is.

430
00:28:47,090 --> 00:28:50,208
What options do we have here, as well as

431
00:28:50,374 --> 00:28:54,370
what a container runtime interface actually means.

432
00:28:57,550 --> 00:29:01,394
As you learned in the lecture about Kubernetes architecture,

433
00:29:01,542 --> 00:29:06,026
on every Kubernetes node there needs to be a container runtime

434
00:29:06,138 --> 00:29:10,138
because the applications in Kubernetes run as containers.

435
00:29:10,314 --> 00:29:14,082
Now, important to note here is that not only our own

436
00:29:14,136 --> 00:29:17,682
applications or databases and other services will

437
00:29:17,736 --> 00:29:21,806
run as containers in Kubernetes, but also Kubernetes

438
00:29:21,918 --> 00:29:25,086
processes themselves, such as API server,

439
00:29:25,198 --> 00:29:29,382
scheduler or controller manager, etCD and so on.

440
00:29:29,516 --> 00:29:33,270
All these are applications, right? And they also run as

441
00:29:33,340 --> 00:29:36,470
containers in Kubernetes. So that means

442
00:29:36,540 --> 00:29:39,830
we need a container runtime on both master

443
00:29:39,910 --> 00:29:43,606
and worker nodes. Now, how do we choose a container

444
00:29:43,638 --> 00:29:48,022
runtime? And how is it that kubernetes doesn't care which runtime

445
00:29:48,086 --> 00:29:52,042
we choose and gives us this flexibility to basically plug

446
00:29:52,106 --> 00:29:55,214
in any container runtime we want? Well,

447
00:29:55,252 --> 00:29:58,926
at the beginning, Kubernetes didn't have this flexibility and

448
00:29:58,948 --> 00:30:03,238
it supported Docker runtime as the first container runtime

449
00:30:03,354 --> 00:30:07,202
by having the code to talk to Docker directly into

450
00:30:07,256 --> 00:30:10,722
the Kubelet code. Remember, Kubelet runs on

451
00:30:10,776 --> 00:30:15,134
every node, master and worker nodes, and talks to Docker

452
00:30:15,262 --> 00:30:18,630
to make it schedule containers, so that

453
00:30:18,700 --> 00:30:22,594
code for talking to Docker was inside the Kubelet

454
00:30:22,642 --> 00:30:26,946
code. However, as more container runtimes were emerging,

455
00:30:27,058 --> 00:30:30,906
Kubernetes wanted to make it more flexible and allow

456
00:30:31,008 --> 00:30:35,238
any container runtime usage with Kubernetes. But integrating

457
00:30:35,334 --> 00:30:38,922
code to talk to all these different runtimes would have been

458
00:30:38,976 --> 00:30:42,782
time consuming and increased the code maintenance effort for

459
00:30:42,836 --> 00:30:46,554
Kubernetes developers because you would need to have separate

460
00:30:46,602 --> 00:30:49,390
code for each container runtime.

461
00:30:49,810 --> 00:30:54,190
Instead, Kubernetes decided to create a single generic

462
00:30:54,270 --> 00:30:58,302
interface where any container runtime who implemented

463
00:30:58,366 --> 00:31:02,210
that interface could be plugged into. And this

464
00:31:02,280 --> 00:31:05,882
plugin interface is what's called the container

465
00:31:05,966 --> 00:31:09,762
runtime interface. So container runtime interface,

466
00:31:09,826 --> 00:31:13,458
or CRI, is basically a set of rules

467
00:31:13,554 --> 00:31:17,000
that defines what a container runtime technology

468
00:31:17,370 --> 00:31:21,434
must implement and how it should implement it in order

469
00:31:21,472 --> 00:31:25,942
for it to be pluggable to Kubernetes as a container runtime.

470
00:31:26,086 --> 00:31:29,194
And through this interface, Kubelet will be able

471
00:31:29,232 --> 00:31:32,778
to talk to it to schedule containers, pull images,

472
00:31:32,874 --> 00:31:36,046
et cetera. Now, Docker, which was the

473
00:31:36,068 --> 00:31:40,042
first container runtime, did not implement CRI

474
00:31:40,106 --> 00:31:44,862
rules, so Kubernetes developers needed to now reintegrate

475
00:31:45,006 --> 00:31:48,654
Docker with Kubelet using the CRi.

476
00:31:48,782 --> 00:31:52,254
How did they do that? Well, they created a Cri

477
00:31:52,302 --> 00:31:56,154
compatible layer in their code for Docker

478
00:31:56,222 --> 00:31:59,490
specifically, which is called Docker shim.

479
00:31:59,650 --> 00:32:03,042
And shim basically means a bridge or connection

480
00:32:03,106 --> 00:32:06,962
between two things. So Docker Shim is part of Kubernetes

481
00:32:07,026 --> 00:32:10,602
code, which lets Kubelet talk to Docker using

482
00:32:10,656 --> 00:32:13,450
the container runtime interface API.

483
00:32:13,790 --> 00:32:17,462
Now, as I mentioned, Docker was the first, most popular

484
00:32:17,526 --> 00:32:21,542
container runtime and important one to be supported.

485
00:32:21,686 --> 00:32:24,522
But it's more than just a container runtime.

486
00:32:24,666 --> 00:32:27,802
Docker is also used to build the images.

487
00:32:27,946 --> 00:32:31,406
So whenever you write an application that needs to be deployed as a

488
00:32:31,428 --> 00:32:34,626
container, you build a Docker image out

489
00:32:34,648 --> 00:32:38,578
of it. It also has its own command line interface and

490
00:32:38,664 --> 00:32:41,986
user interface, so container runtime is

491
00:32:42,008 --> 00:32:45,540
just one part of the Docker technology.

492
00:32:46,090 --> 00:32:49,314
But Kubernetes only needs the container runtime

493
00:32:49,362 --> 00:32:53,174
part of Docker. It doesn't actually need any other parts. And with

494
00:32:53,212 --> 00:32:57,234
the time more lightweight and more fitting container runtimes

495
00:32:57,282 --> 00:33:00,826
emerged that didn't have all these extra

496
00:33:01,008 --> 00:33:04,458
components that Kubernetes didn't actually need. And two of

497
00:33:04,464 --> 00:33:07,958
the most popular container runtimes that emerged

498
00:33:08,134 --> 00:33:11,654
are container D and cry O.

499
00:33:11,792 --> 00:33:16,090
So Docker was not the number one runtime option anymore.

500
00:33:16,250 --> 00:33:20,286
And also Kubernetes team had to continue maintaining the

501
00:33:20,308 --> 00:33:23,954
Docker sheam code to keep Docker support, even though

502
00:33:23,992 --> 00:33:28,302
it doesn't implement the CRI standard like other runtimes.

503
00:33:28,446 --> 00:33:32,578
So Kubernetes recently decided to deprecate and stop

504
00:33:32,664 --> 00:33:36,774
maintaining the Docker shim in version 120

505
00:33:36,892 --> 00:33:41,750
and then completely remove it in version 1.22,

506
00:33:41,900 --> 00:33:45,586
which would mean Kubernetes will not directly support Docker

507
00:33:45,618 --> 00:33:49,430
runtime anymore. Now, does that mean you can't use

508
00:33:49,500 --> 00:33:53,594
Docker in kubernetes from version 122?

509
00:33:53,792 --> 00:33:57,542
No, because if Docker itself or some other company decided

510
00:33:57,606 --> 00:34:01,370
to maintain the Docker shim code or create a service

511
00:34:01,520 --> 00:34:04,854
similar to that, you can deploy that service with

512
00:34:04,912 --> 00:34:08,846
Docker and it will still work. However, it's better to

513
00:34:08,868 --> 00:34:12,650
use more lightweight container runtimes instead of Docker,

514
00:34:12,810 --> 00:34:16,526
especially because you can still pull and run docker

515
00:34:16,558 --> 00:34:20,194
images with other container runtimes. So you can be

516
00:34:20,232 --> 00:34:23,922
creating a bunch of docker images for your applications and running

517
00:34:23,976 --> 00:34:28,082
them in Kubernetes cluster that uses a container D runtime

518
00:34:28,146 --> 00:34:31,990
and that will work just fine. So there is really no reason

519
00:34:32,060 --> 00:34:35,910
to use Docker as a container runtime anymore, and that's what

520
00:34:35,980 --> 00:34:39,078
all cloud platforms are doing. Aws,

521
00:34:39,174 --> 00:34:42,966
Google Cloud, Azure cloud, et cetera all use container

522
00:34:42,998 --> 00:34:47,690
D as a container runtime in their managed Kubernetes clusters.

523
00:34:48,190 --> 00:34:51,722
And that's also what we're going to do. We're going to install and

524
00:34:51,776 --> 00:34:55,706
use container D as a runtime in our cluster,

525
00:34:55,898 --> 00:34:59,614
which is just as easy to install as Docker. So there should be no

526
00:34:59,652 --> 00:35:03,134
big difference there. The only thing would be that we

527
00:35:03,172 --> 00:35:06,926
won't be able to execute Docker commands on Kubernetes nodes

528
00:35:07,038 --> 00:35:10,690
to check and troubleshoot the containers. But again,

529
00:35:10,760 --> 00:35:14,418
in most cases that shouldn't be necessary since we're going

530
00:35:14,424 --> 00:35:18,146
to be working on a Kubernetes level using Kubernetes tools

531
00:35:18,258 --> 00:35:20,470
to troubleshoot applications.

532
00:35:24,260 --> 00:35:28,400
Interface is and also we decided to

533
00:35:28,470 --> 00:35:32,240
install container D runtime on our cluster.

534
00:35:32,320 --> 00:35:35,764
So how do we install it again? You as a

535
00:35:35,802 --> 00:35:38,916
Kubernetes administrator, you are trying to work through the

536
00:35:38,938 --> 00:35:42,952
documentation. So you click on container runtimes link

537
00:35:43,006 --> 00:35:46,996
here and you see those three runtimes are listed.

538
00:35:47,108 --> 00:35:51,156
Click on container D and you have the installation guide for container

539
00:35:51,188 --> 00:35:53,610
D for your cluster right here.

540
00:35:56,320 --> 00:36:00,076
So this is exactly what we're going to do and we're going

541
00:36:00,098 --> 00:36:04,024
to execute these commands one by one. So going back to my master

542
00:36:04,072 --> 00:36:08,540
node, execute the first command which basically

543
00:36:08,690 --> 00:36:12,400
creates this container D configuration file with the list

544
00:36:12,470 --> 00:36:15,996
of necessary modules that need to be loaded for container

545
00:36:16,028 --> 00:36:19,616
D. And after that we execute these two commands which

546
00:36:19,638 --> 00:36:23,456
will then actually load those container

547
00:36:23,488 --> 00:36:27,156
D modules. You don't have to understand these in detail. That's why you

548
00:36:27,178 --> 00:36:30,564
have these installation guides usually wherever you need

549
00:36:30,602 --> 00:36:34,676
them, so you can just copy and paste them. So let's grab

550
00:36:34,788 --> 00:36:38,504
the next one which again creates a

551
00:36:38,542 --> 00:36:42,724
configuration file for Kubernetes

552
00:36:42,852 --> 00:36:46,828
cri config, which very simply explain

553
00:36:46,914 --> 00:36:50,060
just sets these three

554
00:36:50,130 --> 00:36:53,868
parameters that we need for container D. And finally

555
00:36:53,954 --> 00:36:57,724
to apply those parameters we are going to execute this

556
00:36:57,762 --> 00:37:01,196
command. And there you go, you see also an output here applying

557
00:37:01,308 --> 00:37:05,036
the config file that we created with those kernel

558
00:37:05,148 --> 00:37:08,972
parameters. So this will take care of configuring

559
00:37:09,036 --> 00:37:12,420
the prerequisites for container D before we install

560
00:37:12,490 --> 00:37:16,496
that. And after that we are going to actually install container

561
00:37:16,528 --> 00:37:20,260
D and apply the configuration with these steps.

562
00:37:22,940 --> 00:37:26,452
So first of all, install the container d package

563
00:37:26,596 --> 00:37:30,920
and we can do that very simply by simply using

564
00:37:31,070 --> 00:37:34,516
our Aptget package manager on ubuntu. So let's

565
00:37:34,548 --> 00:37:38,220
clear this up and let's do pseudo aptget

566
00:37:38,640 --> 00:37:42,632
update to first update the repositories.

567
00:37:42,776 --> 00:37:46,632
And after that we're going to do pseudo

568
00:37:46,696 --> 00:37:51,500
aptget install container

569
00:37:51,580 --> 00:37:55,600
D and let's execute confirm

570
00:37:56,020 --> 00:38:00,196
and this will install container d application as

571
00:38:00,218 --> 00:38:04,112
a next step, we're creating a containerd configuration folder

572
00:38:04,176 --> 00:38:07,536
in etc. And then inside that folder we're

573
00:38:07,568 --> 00:38:10,944
creating this default configuration containerd

574
00:38:10,992 --> 00:38:14,492
config default. This gives us default configuration for containerd

575
00:38:14,576 --> 00:38:18,100
and we are saving that into that file. So let's

576
00:38:18,260 --> 00:38:19,690
execute those two.

577
00:38:21,820 --> 00:38:25,544
And finally we restart the

578
00:38:25,582 --> 00:38:29,448
running container D with the updated configuration.

579
00:38:29,624 --> 00:38:33,528
And now container D actually should be running on our node

580
00:38:33,624 --> 00:38:37,356
and of course we would like to check that. So for that

581
00:38:37,378 --> 00:38:41,200
I'm going to do service container d status

582
00:38:42,020 --> 00:38:46,048
and this will give us the status of container d, which is

583
00:38:46,134 --> 00:38:49,168
active in a running state. And you also see

584
00:38:49,254 --> 00:38:52,532
some of the logs on startup, no errors here,

585
00:38:52,586 --> 00:38:56,432
everything looks fine. Alternatively, you can also check the status

586
00:38:56,496 --> 00:39:00,628
of a running process on the server using system control

587
00:39:00,714 --> 00:39:01,460
command,

588
00:39:03,820 --> 00:39:07,784
system control status and then the service name which

589
00:39:07,822 --> 00:39:11,316
is again container d and you get basically the same output

590
00:39:11,348 --> 00:39:14,968
here. So both commands work the same way.

591
00:39:15,054 --> 00:39:18,604
And as you already know, container d process needs to be

592
00:39:18,642 --> 00:39:21,560
installed on every single node in the cluster.

593
00:39:21,720 --> 00:39:25,820
So we are going to install that on worker one and worker two

594
00:39:25,890 --> 00:39:29,612
as well. But to make our work a little bit easier, instead of going

595
00:39:29,666 --> 00:39:32,928
through these commands one by one, again we're just going

596
00:39:32,934 --> 00:39:36,544
to create a script basically that will just execute in

597
00:39:36,582 --> 00:39:40,520
one step so we don't have to type out the commands.

598
00:39:40,620 --> 00:39:44,710
So I'm going to copy this first and let's create an install

599
00:39:45,160 --> 00:39:48,436
sh. I'm going to

600
00:39:48,458 --> 00:39:51,648
paste the whole thing in there. After setting

601
00:39:51,664 --> 00:39:55,400
these prerequisites and configuration, we then

602
00:39:55,470 --> 00:39:59,156
installed the container d using package

603
00:39:59,188 --> 00:40:02,920
manager. So we did pseudo aptget

604
00:40:03,260 --> 00:40:06,820
update pseudo aptget

605
00:40:06,980 --> 00:40:18,652
install container d and

606
00:40:18,706 --> 00:40:22,612
there you go. And finally we can also print out the

607
00:40:22,666 --> 00:40:26,596
status of container d at

608
00:40:26,618 --> 00:40:30,292
the end of the script execution. And there is one thing we need to do

609
00:40:30,346 --> 00:40:34,836
in an automated script which is manually confirm

610
00:40:34,948 --> 00:40:38,280
the installation of a package. So basically

611
00:40:38,350 --> 00:40:42,216
we need to add this y option here

612
00:40:42,318 --> 00:40:45,868
so that it gets automatically confirmed and the script will continue.

613
00:40:46,034 --> 00:40:50,552
And basically that's our container D installation script.

614
00:40:50,696 --> 00:40:55,016
We're going to save this install sh file

615
00:40:55,048 --> 00:40:58,576
by default is not executable because it

616
00:40:58,598 --> 00:41:02,016
doesn't have an execute or x permission here.

617
00:41:02,118 --> 00:41:05,568
So to make it executable we're going

618
00:41:05,574 --> 00:41:10,612
to add an execute permission and

619
00:41:10,666 --> 00:41:13,620
we are going to edit only for our user.

620
00:41:14,920 --> 00:41:18,212
And let's see now we can execute that

621
00:41:18,266 --> 00:41:21,756
script. Also have color coding for executable files

622
00:41:21,888 --> 00:41:24,650
and now we can do install sh.

623
00:41:27,100 --> 00:41:31,364
Everything got successfully executed and we see the active

624
00:41:31,412 --> 00:41:35,016
status of container D started. Container D container

625
00:41:35,048 --> 00:41:38,556
runtime. Awesome. To exit from here we can

626
00:41:38,578 --> 00:41:41,820
just click letter q and it will

627
00:41:41,890 --> 00:41:45,176
quit the output. So basically we have a container

628
00:41:45,208 --> 00:41:49,840
D running here as well. Let's copy

629
00:41:50,580 --> 00:41:56,528
the install sh contents for

630
00:41:56,614 --> 00:41:59,410
worker node two as well.

631
00:42:00,040 --> 00:42:02,790
Let's paste them in there, save it,

632
00:42:03,160 --> 00:42:05,940
do the same thing, make it executable,

633
00:42:07,400 --> 00:42:10,644
and execute the

634
00:42:10,682 --> 00:42:13,800
script. Cool. So the first step

635
00:42:13,870 --> 00:42:17,716
of cluster installation is done. We have a container

636
00:42:17,748 --> 00:42:20,824
runtime container D running on every

637
00:42:20,942 --> 00:44:10,364
single cluster node to

638
00:44:10,402 --> 00:44:14,028
install the Kubernetes processes on

639
00:44:14,114 --> 00:44:18,524
master and worker nodes. And as I mentioned in

640
00:44:18,642 --> 00:44:22,244
one of the previous lectures, we will use the Kubeadm

641
00:44:22,312 --> 00:44:26,444
tool to deploy those processes. So how does Kubeadm

642
00:44:26,492 --> 00:44:30,448
actually work? First, we install Kubeadm tool on each

643
00:44:30,534 --> 00:44:34,280
of the nodes, just like we installed the container runtime.

644
00:44:34,380 --> 00:44:38,500
Once it's available on the servers, we can execute different

645
00:44:38,570 --> 00:44:42,176
commands with it. The very first command we execute

646
00:44:42,288 --> 00:44:45,968
with Kubeadm is Kubeadm init command,

647
00:44:46,064 --> 00:44:49,576
and this command will be executed only once on

648
00:44:49,678 --> 00:44:53,732
one of the master nodes. Remember, master node at the beginning

649
00:44:53,796 --> 00:44:56,904
is just an empty server, and we turn it into

650
00:44:56,942 --> 00:45:00,120
a control plane or master node by executing

651
00:45:00,200 --> 00:45:04,076
Kubeadm init on it. This command will basically

652
00:45:04,178 --> 00:45:07,292
orchestrate the whole cluster setup. It will

653
00:45:07,346 --> 00:45:11,256
generate etc folder

654
00:45:11,368 --> 00:45:15,020
and inside that will put all the generated certificates,

655
00:45:15,180 --> 00:45:19,356
all the Kubernetes manifest files for Kubernetes applications.

656
00:45:19,468 --> 00:45:22,752
It will also create all the necessary configurations for these

657
00:45:22,806 --> 00:45:25,664
applications like API server, kubelet,

658
00:45:25,712 --> 00:45:29,444
etcd and so on. When the manifest files get

659
00:45:29,562 --> 00:45:33,248
generated and put into the etC Kubernetes

660
00:45:33,424 --> 00:45:37,124
manifests folder, Kubelet, which looks for any

661
00:45:37,162 --> 00:45:41,032
static pod manifests in that location, will detect them and

662
00:45:41,086 --> 00:45:44,292
start those static pods with the suffix

663
00:45:44,356 --> 00:45:47,704
of the hostname. Kubelet will ask container d

664
00:45:47,742 --> 00:45:51,212
runtime to fetch all the images for

665
00:45:51,266 --> 00:45:54,924
the applications in those manifests and to

666
00:45:54,962 --> 00:45:58,444
eventually schedule them as containers. So at the end

667
00:45:58,482 --> 00:46:02,072
we will have all the processes up and running and Kubernetes

668
00:46:02,136 --> 00:46:05,664
cluster will have been initialized with just one

669
00:46:05,702 --> 00:46:08,684
node. So we're going to have a one node cluster,

670
00:46:08,732 --> 00:46:12,512
basically. So we need to install Kubeadm on our server now.

671
00:46:12,646 --> 00:46:16,016
But an important point here is that Kubeadm

672
00:46:16,128 --> 00:46:20,032
does not install and manage Kubelet,

673
00:46:20,176 --> 00:46:23,712
which means before we initialize the cluster,

674
00:46:23,856 --> 00:46:27,096
we need to install Kubelet as well. And finally,

675
00:46:27,198 --> 00:46:31,016
when we create the cluster, we need to interact with

676
00:46:31,038 --> 00:46:34,168
it to administer it, right? So we're going to install

677
00:46:34,254 --> 00:46:38,660
the third application, which is a Kubernetes client

678
00:46:38,740 --> 00:46:43,064
command line tool called Kubectl. So again, for comparison

679
00:46:43,112 --> 00:46:46,700
here, if you got confused by those similar

680
00:46:46,850 --> 00:46:50,604
names of the three applications, Kubelet is

681
00:46:50,642 --> 00:46:54,700
a process that we need to run pods on master

682
00:46:54,780 --> 00:46:58,672
and worker nodes equally. Kubeadm is a command line tool

683
00:46:58,726 --> 00:47:01,904
that we use to initiate or create the

684
00:47:01,942 --> 00:47:05,568
Cluster and Kubectl is a command line tool

685
00:47:05,734 --> 00:47:09,604
that we use to work with the cluster once it has been

686
00:47:09,642 --> 00:47:12,868
created. So we're going to use Kubeadm only once

687
00:47:12,954 --> 00:47:16,212
at the beginning to initiate the cluster. And after

688
00:47:16,266 --> 00:47:20,228
that Kubectl is going to be our main Kubernetes

689
00:47:20,324 --> 00:47:24,548
tool that we will be working with to create Kubernetes components,

690
00:47:24,644 --> 00:47:27,412
get information about these components, etc.

691
00:47:27,556 --> 00:47:31,164
And all these three tools or applications are

692
00:47:31,202 --> 00:47:34,568
maintained by kubernetes and have the same release

693
00:47:34,664 --> 00:47:38,076
versions. So let's install them again. If we

694
00:47:38,098 --> 00:47:41,288
go back to the documentation after installing runtime,

695
00:47:41,384 --> 00:47:44,332
that's the next step to install these packages,

696
00:47:44,476 --> 00:47:48,144
Kubeadm, Kubelet and Kubectl. And installing them is

697
00:47:48,182 --> 00:47:51,504
also very straightforward. We just need to follow the instructions here.

698
00:47:51,622 --> 00:47:54,724
Choose the instructions for our operating system.

699
00:47:54,842 --> 00:47:58,000
This one is for debian based distributions like Ubuntu.

700
00:47:58,080 --> 00:48:00,390
So that's the guide we use.

701
00:48:03,240 --> 00:48:06,636
So the first step is before we install Kubelet,

702
00:48:06,688 --> 00:48:10,532
Kubeadm and Kubectl to basically configure

703
00:48:10,596 --> 00:48:14,852
the Kubernetes repository that holds those packages.

704
00:48:14,996 --> 00:48:19,424
So we need to connect the apt package manager on Ubuntu to Kubernetes

705
00:48:19,572 --> 00:48:22,940
apt repository so that we can then

706
00:48:23,010 --> 00:48:26,204
install packages from it. So the first one here

707
00:48:26,242 --> 00:48:30,380
basically just installs packages needed to

708
00:48:30,530 --> 00:48:33,804
set up the Kubernetes repository. So let's copy

709
00:48:33,852 --> 00:48:39,452
them and execute them again starting with master pseudoept

710
00:48:39,516 --> 00:48:43,900
get update and installing those

711
00:48:44,070 --> 00:48:45,220
three packages.

712
00:48:48,920 --> 00:48:53,456
The second step which downloads

713
00:48:53,568 --> 00:48:57,492
Google Cloud public signing key. This is what our package manager

714
00:48:57,556 --> 00:49:01,944
will need to connect to the repository. And finally

715
00:49:02,062 --> 00:49:05,304
we add the repository to our

716
00:49:05,422 --> 00:49:08,856
package manager's repository list. And there

717
00:49:08,878 --> 00:49:17,470
you go again, Aptget update and

718
00:49:18,000 --> 00:49:21,812
this command will actually install Kubelet,

719
00:49:21,976 --> 00:49:25,484
Kubeadm and Kubectl commands.

720
00:49:25,612 --> 00:49:28,592
And since we're not specifying versions here,

721
00:49:28,646 --> 00:49:32,880
this will just download the latest stable versions of

722
00:49:32,950 --> 00:49:37,124
each component. As I said, these three components actually

723
00:49:37,162 --> 00:49:40,468
have the same release cycle so it will be the

724
00:49:40,474 --> 00:49:44,420
same version. But what if you want to install a specific

725
00:49:44,570 --> 00:49:48,664
Kubernetes version? First of all, how do you look up what

726
00:49:48,702 --> 00:49:52,292
versions you have available? And second, how do you then specify

727
00:49:52,356 --> 00:49:56,152
that? Well, let's actually delete that and list

728
00:49:56,206 --> 00:49:59,776
all the versions that we have available for Kubeadm.

729
00:49:59,828 --> 00:50:04,700
For example apt cache

730
00:50:08,000 --> 00:50:11,676
using this command. If I execute this,

731
00:50:11,858 --> 00:50:16,656
you will see all the versions available and

732
00:50:16,678 --> 00:50:20,636
the latest one is this right here. So this will be installed by default

733
00:50:20,668 --> 00:50:24,264
if we don't specify a version. And let's say we don't want the latest

734
00:50:24,332 --> 00:50:27,108
version, we want this specific version right here,

735
00:50:27,194 --> 00:50:29,670
1.210.

736
00:50:30,440 --> 00:50:32,070
So let's copy that.

737
00:50:37,000 --> 00:50:40,856
So that's our version. Let's just execute it, comment it out. So we

738
00:50:40,878 --> 00:50:44,932
have it there. And I'm going to copy the installation

739
00:50:44,996 --> 00:50:48,680
command again. And let's specify that version for

740
00:50:48,750 --> 00:50:53,304
each package. Again, they have the same versioning,

741
00:50:53,432 --> 00:50:57,324
so each one will get the same version and this will install

742
00:50:57,442 --> 00:51:01,550
Kubernetes version 1.210.

743
00:51:01,860 --> 00:51:03,360
And let's execute.

744
00:51:06,100 --> 00:51:08,960
And finally, the last command,

745
00:51:09,620 --> 00:51:13,376
which is EPTM mark hold for each of

746
00:51:13,398 --> 00:51:17,620
the packages. What this basically does is it prevents

747
00:51:18,120 --> 00:51:22,452
these packages from being automatically upgraded or

748
00:51:22,506 --> 00:51:26,116
removed. So it basically fixates the version to

749
00:51:26,218 --> 00:51:29,672
whatever is installed. And this is good

750
00:51:29,726 --> 00:51:33,316
if you want to keep your cluster stable and prevent,

751
00:51:33,508 --> 00:51:37,240
for example, accidental upgrades of any of these packages.

752
00:51:37,740 --> 00:51:41,336
So let's execute that as well. And that's

753
00:51:41,368 --> 00:51:45,496
basically it. We have now Kubelet installed,

754
00:51:45,608 --> 00:51:49,548
which is running as a service. And we can also check that using

755
00:51:49,634 --> 00:51:53,264
service Kubelet status. And you will

756
00:51:53,302 --> 00:51:56,416
see that Kubelet is actually not in a

757
00:51:56,518 --> 00:52:00,256
running state. It says activating and a

758
00:52:00,278 --> 00:52:03,964
failure status, which is actually expected at this stage because

759
00:52:04,102 --> 00:52:07,748
Kubelet doesn't have anything to do at this moment. So basically,

760
00:52:07,914 --> 00:52:12,260
Kubelet is just waiting for Kubeadm to

761
00:52:12,330 --> 00:52:15,424
initialize the cluster and tell Kubelet

762
00:52:15,552 --> 00:52:20,164
what pods to schedule and basically what to do. So once we initiate

763
00:52:20,212 --> 00:52:23,448
and create the cluster, Kubelet will be in a running state.

764
00:52:23,534 --> 00:52:28,596
So we say that Kubelet is installed. We also have Kubeadm

765
00:52:28,788 --> 00:52:32,910
command and

766
00:52:34,400 --> 00:52:38,636
the version that we installed, which is 1.210,

767
00:52:38,818 --> 00:52:43,040
as well as Kubectl version, which is the same.

768
00:52:43,190 --> 00:52:47,136
And note that at this point, we don't have the cluster up

769
00:52:47,158 --> 00:52:51,356
and running. We haven't initiated the cluster, we haven't executed

770
00:52:51,388 --> 00:52:55,056
Kubeadm init command. We have just installed all the

771
00:52:55,078 --> 00:52:58,416
tools that we need for setting up the cluster.

772
00:52:58,528 --> 00:53:02,736
And these are exactly the same tools that we need on worker nodes

773
00:53:02,768 --> 00:53:06,064
as well. So let's go ahead and do the same installation

774
00:53:06,192 --> 00:53:09,080
process on each of the worker nodes.

775
00:53:11,740 --> 00:53:15,224
And I'm going to do it the same way as for container deal.

776
00:53:15,262 --> 00:53:19,720
And let's call this install Kubernetes components

777
00:53:20,640 --> 00:53:29,670
sh and

778
00:53:29,820 --> 00:53:34,550
basically just copy paste those commands.

779
00:53:39,310 --> 00:53:42,490
And here, do not forget the version

780
00:53:42,830 --> 00:53:46,682
because each node of course, needs to have

781
00:53:46,816 --> 00:53:50,426
the same versions. So let's

782
00:53:50,458 --> 00:53:51,920
specify that as well.

783
00:53:58,610 --> 00:54:02,254
And that's it. That's our script for installing Kubernetes

784
00:54:02,302 --> 00:54:07,410
components. Let's also make it executable and

785
00:54:07,560 --> 00:54:08,530
execute.

786
00:54:12,070 --> 00:54:16,470
Everything looks fine. Let's check that Kubelet is

787
00:54:16,540 --> 00:54:19,958
present on the machine as well as

788
00:54:20,044 --> 00:54:23,830
cube, Adm and Kubectl.

789
00:54:24,330 --> 00:54:28,870
Everything looks fine. Let's copy

790
00:54:31,290 --> 00:54:51,060
the installation, script it

791
00:54:55,190 --> 00:54:56,500
and there you go.

792
00:55:03,830 --> 00:55:07,734
Check the kubelet and

793
00:55:07,772 --> 00:55:12,242
its version and it seems like everything was installed successfully

794
00:55:12,386 --> 00:55:16,310
and now we actually have the same exact state on all

795
00:55:16,380 --> 00:55:19,906
our nodes. So we have container

796
00:55:19,938 --> 00:55:23,434
d running on each node. We have Kubelet running and

797
00:55:23,472 --> 00:55:27,126
waiting for Kubeadm to basically give it some commands

798
00:55:27,158 --> 00:55:31,626
to schedule pods. And we have Kubeadm and Kubectl

799
00:55:31,738 --> 00:55:35,946
command line tools installed on each of the nodes.

800
00:55:36,058 --> 00:55:40,254
So everything is present on the machines to

801
00:55:40,372 --> 00:55:43,634
create a cluster. And now it's actually time to turn

802
00:55:43,672 --> 00:55:47,186
this server into a master node using

803
00:55:47,288 --> 00:55:49,490
kubeadm init command.

804
00:55:59,370 --> 00:56:03,230
Execute pseudo kubeadm

805
00:56:03,330 --> 00:56:06,806
init command. And we need pseudo here because Kubeadm

806
00:56:06,838 --> 00:56:10,646
will actually write the generated configuration files

807
00:56:10,758 --> 00:56:14,394
into etcubernetes folder which

808
00:56:14,432 --> 00:56:17,854
is accessible only for root user. So we

809
00:56:17,892 --> 00:56:21,738
need the root permission to execute the init command.

810
00:56:21,834 --> 00:56:25,822
So let's do that. And now Kubeadm is actually

811
00:56:25,956 --> 00:56:29,394
doing everything that we went through in the

812
00:56:29,432 --> 00:56:32,846
installation overview lecture and basically bootstrapping

813
00:56:32,878 --> 00:56:34,770
the whole cluster in the background.

814
00:56:36,230 --> 00:56:39,970
Awesome. We just initialized a Kubernetes cluster and

815
00:56:40,040 --> 00:56:43,750
created a master node and there are a bunch of things that

816
00:56:43,820 --> 00:56:47,378
happened in the background that we also see in the output.

817
00:56:47,474 --> 00:56:50,694
So let's check out some of the interesting stuff here.

818
00:56:50,812 --> 00:56:53,958
First of all we have this pre flight stage.

819
00:56:54,054 --> 00:56:57,366
So Kubeadm executes everything in different stages

820
00:56:57,478 --> 00:57:01,066
in pre flight stage. Basically all the checks will

821
00:57:01,088 --> 00:57:04,426
be made to ensure that our servers are in

822
00:57:04,448 --> 00:57:08,666
the proper state so that Kubernetes cluster can be installed

823
00:57:08,698 --> 00:57:12,462
on it. Another thing that is done in preflight mode is

824
00:57:12,596 --> 00:57:17,022
downloading or pulling all the images that are needed

825
00:57:17,156 --> 00:57:20,354
to start the Kubernetes components and to set

826
00:57:20,392 --> 00:57:24,226
up the cluster. The next phase as you see is

827
00:57:24,328 --> 00:57:28,018
certificates or certs stage and this is where

828
00:57:28,104 --> 00:57:32,342
generating all the necessary certificates takes

829
00:57:32,396 --> 00:57:35,640
place. The first step is creating this

830
00:57:36,170 --> 00:57:40,002
etcuberpki

831
00:57:40,146 --> 00:57:43,546
folder. This is where all the generated certificates will

832
00:57:43,568 --> 00:57:46,570
be put and we can actually check that folder.

833
00:57:51,470 --> 00:57:54,874
As you see we have a bunch of certificates here. We have

834
00:57:54,992 --> 00:57:58,366
the central component which is API server has the

835
00:57:58,388 --> 00:58:02,558
server certificate and private key which is used for others

836
00:58:02,644 --> 00:58:06,826
or clients to connect and authenticate. API server

837
00:58:06,938 --> 00:58:10,378
the API server also has client certificates

838
00:58:10,474 --> 00:58:13,634
to talk to HCD and Kubelet services.

839
00:58:13,832 --> 00:58:17,394
And you also see that CA certificates were

840
00:58:17,512 --> 00:58:20,894
generated. So this is a self signed CA

841
00:58:21,032 --> 00:58:24,662
of Kubernetes which basically signs all the other

842
00:58:24,796 --> 00:58:28,146
certificates and inside that you also see Etsy

843
00:58:28,258 --> 00:58:32,434
folder. So ECD basically has its own certificate

844
00:58:32,562 --> 00:58:37,122
authority and it has its own server certificates

845
00:58:37,266 --> 00:58:41,290
because ETCD doesn't actually talk to anyone other than itself

846
00:58:41,360 --> 00:58:45,574
or its peers. If we have multiple ETCD replicas,

847
00:58:45,702 --> 00:58:49,514
it only has the server certificate so that API server

848
00:58:49,562 --> 00:58:53,022
can talk to it. So as you see, ETCD has its

849
00:58:53,076 --> 00:58:56,702
own CA to sign its server certificates and

850
00:58:56,756 --> 00:59:00,746
API server has its own CA to sign its server

851
00:59:00,778 --> 00:59:04,674
certificates. So that's the certs phase. Then we

852
00:59:04,712 --> 00:59:08,354
have Kubeconfig generation phase where basically

853
00:59:08,552 --> 00:59:12,226
for every client that needs to talk to API

854
00:59:12,258 --> 00:59:16,134
server, Kubeconfig files get created. Again, we can see

855
00:59:16,172 --> 00:59:19,830
that in Kubernetes folder,

856
00:59:21,850 --> 00:59:25,370
so we were in the PKI folder, but inside

857
00:59:25,440 --> 00:59:28,890
this Kubernetes folder we have all these

858
00:59:28,960 --> 00:59:32,234
Kubeconfig files for each of the clients that

859
00:59:32,272 --> 00:59:35,622
are connecting to the API server controller manager,

860
00:59:35,686 --> 00:59:38,966
Kubelet scheduler and admin user

861
00:59:38,998 --> 00:59:42,902
which we can use to connect to the cluster and administer

862
00:59:42,966 --> 00:59:46,650
it. Another important thing that Kubeadm does

863
00:59:46,720 --> 00:59:51,054
in the background is configuring the Kubelet environment.

864
00:59:51,182 --> 00:59:54,834
So you remember that Kubelet was in a state of basically

865
00:59:54,952 --> 00:59:58,434
activating and waiting for Kubeadm to give

866
00:59:58,472 --> 00:59:59,920
it instructions of what to do.

